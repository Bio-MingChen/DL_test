{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These codes come from TIGON  project (https://github.com/yutongo/TIGON/blob/19d6648195a47b4d2a2d5025b440d37cf0ac9a17/AE/models.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "# from torchvision import datasets, transforms\n",
    "import collections\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from AE.utility import create_activation,compute_distance_matrix\n",
    "sys.path.append('../')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers_list, dropout, norm,activation,last_act=False):\n",
    "        super(MLP, self).__init__()\n",
    "        layers=nn.ModuleList()\n",
    "        assert len(layers_list)>=2, 'no enough layers'\n",
    "        for i in range(len(layers_list)-2):\n",
    "            layers.append(nn.Linear(layers_list[i],layers_list[i+1]))\n",
    "            if norm:\n",
    "                layers.append(nn.BatchNorm1d(layers_list[i+1]))\n",
    "            if activation is not None:\n",
    "                layers.append(activation)\n",
    "            if dropout>0.:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(layers_list[-2],layers_list[-1]))\n",
    "        if norm:\n",
    "            layers.append(nn.BatchNorm1d(layers_list[-1]))\n",
    "        if last_act:\n",
    "            if activation is not None:\n",
    "                layers.append(activation)\n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        # layers.append(nn.Linear(layers_list[-1],out_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        # self.apply(init_weights_xavier_uniform)\n",
    "    def forward(self,x):\n",
    "        for layer in self.network:\n",
    "            x=layer(x)\n",
    "        return x\n",
    "class LatentModel(nn.Module):\n",
    "    def __init__(self,n_hidden,n_latent,\n",
    "                 kl_weight=1e-6, warmup_step=10000):\n",
    "        super(LatentModel,self).__init__()\n",
    "        self.mu = nn.Linear(n_hidden, n_latent)\n",
    "        self.logvar = nn.Linear(n_hidden, n_latent)\n",
    "        # self.kl = 0\n",
    "        self.kl_weight = kl_weight\n",
    "        self.step_count = 0\n",
    "        self.warmup_step = warmup_step\n",
    "\n",
    "    def kl_schedule_step(self):\n",
    "        self.step_count += 1\n",
    "        if self.step_count < self.warmup_step:\n",
    "            self.kl_weight = 0.0\n",
    "        else:\n",
    "            self.kl_weight = self.kl_weight + (1e-2 - 1e-6) / self.warmup_step\n",
    "\n",
    "        # elif self.step_count == self.warmup_step:\n",
    "        #     pass\n",
    "            # self.step_count = 0\n",
    "            # self.kl_weight = 1e-6\n",
    "\n",
    "    def forward(self, h):\n",
    "        mu = self.mu(h)\n",
    "        log_var = self.logvar(h)\n",
    "        sigma = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        if self.training:\n",
    "            z = mu + sigma * epsilon\n",
    "            # (1 + log_var - mu ** 2 - log_var.exp()).sum()* self.kl_weight\n",
    "            # print('hhhhhhh')\n",
    "            self.kl = -0.5 * (1 + log_var - mu ** 2 - log_var.exp()).sum()  * self.kl_weight#/ z.shape[0]\n",
    "            self.kl_schedule_step()\n",
    "        else:\n",
    "            z = mu\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            n_layers: int = 1,\n",
    "            n_hidden: int = 500,\n",
    "            n_latent: int = 10,\n",
    "            activate_type: str='relu',\n",
    "            dropout: float = 0.2,\n",
    "            norm: bool = False,\n",
    "            seed: int=42,\n",
    "    ):\n",
    "        '''\n",
    "        Autoencoder model.\n",
    "        Encoder and Decoder take identical architectures.\n",
    "\n",
    "        Parameters:\n",
    "            in_dim:\n",
    "                dimension of the input feature\n",
    "            n_layers:\n",
    "                number of hidden layers\n",
    "            n_hidden:\n",
    "                dimension of hidden layer. All hidden layers take the same dimensions\n",
    "            n_latent:\n",
    "                dimension of latent space\n",
    "            activate_type:\n",
    "                activation functions.\n",
    "                Options: 'leakyrelu','relu', 'gelu', 'prelu', 'elu', and None for identity map.\n",
    "            dropout:\n",
    "                dropout rate\n",
    "            norm:\n",
    "                whether to include batch normalization layer\n",
    "            seed:\n",
    "                random seed.\n",
    "        '''\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        self.in_dim=in_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.n_hidden=n_hidden\n",
    "        self.n_latent=n_latent\n",
    "        self.dropout=dropout\n",
    "        self.norm=norm\n",
    "        self.activation=create_activation(activate_type)\n",
    "        ## Encoder:\n",
    "        self.encoder_layer=[in_dim]\n",
    "        for i in range(n_layers):\n",
    "            self.encoder_layer.append(n_hidden)\n",
    "        self.encoder=MLP(self.encoder_layer,dropout,norm,self.activation,last_act=True)\n",
    "        self.encoder_to_latent=MLP([self.encoder_layer[-1],n_latent],\n",
    "                                   dropout,norm,self.activation)\n",
    "\n",
    "        ## Decoder:\n",
    "        self.decoder_layer=[n_latent]\n",
    "        for i in range(n_layers):\n",
    "            self.decoder_layer.append(n_hidden)\n",
    "        self.decoder=MLP(self.decoder_layer,dropout,norm,self.activation,last_act=True)\n",
    "        self.decoder_to_output=MLP([self.decoder_layer[-1],self.in_dim],dropout,norm,activation=None)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        rep=self.get_latent_representation(x,tensor=True)\n",
    "        h = self.decoder(rep)\n",
    "        x_recon=self.decoder_to_output(h)\n",
    "        mse = nn.MSELoss(reduction='sum')\n",
    "        loss = mse(x_recon, x)/x.shape[1]\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def get_latent_representation(self,x,tensor:bool=False):\n",
    "        '''\n",
    "        Get latent space representation\n",
    "\n",
    "        Parameters\n",
    "        x:\n",
    "            Input space\n",
    "        tensor:\n",
    "            If input x is a tensor, or it is a numpy array\n",
    "        Return\n",
    "        rep:\n",
    "            latent space representation\n",
    "            If tensor==True:\n",
    "                return a tensor\n",
    "            If tensor==Flase:\n",
    "                return a numpy array\n",
    "        '''\n",
    "#        if not tensor:\n",
    "#            x=torch.tensor(x,dtype=torch.float32)\n",
    "#            self.eval()\n",
    "        x=self.encoder(x)\n",
    "        rep=self.encoder_to_latent(x)\n",
    "        #if not tensor:\n",
    "        #    rep=rep.detach().numpy()\n",
    "        return rep\n",
    "    def get_reconstruction(self, x):\n",
    "        '''\n",
    "        Reconstruct/impute gene expression data\n",
    "        x:\n",
    "            features. Numpy array\n",
    "        Return\n",
    "        x_recon:\n",
    "            Numpy array\n",
    "        '''\n",
    "        self.eval()\n",
    "        x=torch.tensor(x,dtype=torch.float32)\n",
    "#        with torch.no_grad():\n",
    "        x=self.encoder(x)\n",
    "        x=self.encoder_to_latent(x)\n",
    "        x = self.decoder(x)\n",
    "        x_recon = self.decoder_to_output(x)\n",
    "\n",
    "        #x_recon=x_recon.detach().numpy()\n",
    "        return x_recon\n",
    "    def get_generative(self,z):\n",
    "        '''\n",
    "        genereate gene expression data from latent space variable\n",
    "        z:\n",
    "            latent space representation. Numpy array\n",
    "        Return\n",
    "        x_recon:\n",
    "            Numpy array\n",
    "        '''\n",
    "        self.eval()\n",
    "        #z=torch.tensor(z,dtype=torch.float32)\n",
    "#        with torch.no_grad():\n",
    "        x = self.decoder(z)\n",
    "        x_recon = self.decoder_to_output(x)\n",
    "        #x_recon=x_recon.detach().numpy()\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个自编码器（Autoencoder, AE）和一些相关的模型架构，如 `MLP` 和 `LatentModel`。下面我将逐步解析每个函数的功能和具体实现。\n",
    "\n",
    "### 1. **`MLP` 类**\n",
    "- **功能**: 定义了一个多层感知机（MLP）模型。\n",
    "- **`__init__` 方法**:\n",
    "  - `layers_list`: 一个列表，定义了每一层的维度。\n",
    "  - `dropout`: Dropout层的丢弃率，用于防止过拟合。\n",
    "  - `norm`: 是否添加Batch Normalization层。\n",
    "  - `activation`: 激活函数（如ReLU, Sigmoid等）。\n",
    "  - `last_act`: 是否在最后一层添加激活函数。\n",
    "  - `self.network`: 将这些层和操作堆叠在一起，形成一个完整的神经网络。\n",
    "\n",
    "- **`forward` 方法**:\n",
    "  - 依次通过网络中的各层，返回网络的输出。\n",
    "\n",
    "### 2. **`LatentModel` 类**\n",
    "- **功能**: 定义了一个变分自编码器（VAE）中的潜在空间模型，生成潜在向量并计算KL散度（用于变分推断）。\n",
    "- **`__init__` 方法**:\n",
    "  - `n_hidden`: 隐藏层的维度。\n",
    "  - `n_latent`: 潜在空间的维度。\n",
    "  - `kl_weight`: KL散度的权重。\n",
    "  - `warmup_step`: 训练过程中KL散度的预热步数。\n",
    "\n",
    "- **`kl_schedule_step` 方法**:\n",
    "  - 用于逐步增加KL散度的权重，直到预热阶段结束。\n",
    "\n",
    "- **`forward` 方法**:\n",
    "  - 根据输入的隐藏层表示 `h`，计算潜在变量的均值 `mu` 和对数方差 `log_var`。\n",
    "  - 如果模型处于训练模式，生成潜在向量 `z` 并计算KL散度。\n",
    "  - 在推理模式下，直接使用均值 `mu` 作为潜在变量。\n",
    "\n",
    "### 3. **`AutoEncoder` 类**\n",
    "- **功能**: 定义了一个标准的自编码器（Autoencoder）。在训练过程中，通过最小化输入数据与重构数据之间的均方误差（MSE）来训练模型。\n",
    "- **`__init__` 方法**:\n",
    "  - `in_dim`: 输入数据的维度。\n",
    "  - `n_layers`: 隐藏层的层数。\n",
    "  - `n_hidden`: 隐藏层的维度。\n",
    "  - `n_latent`: 潜在空间的维度。\n",
    "  - `activate_type`: 激活函数类型（如ReLU, LeakyReLU等）。\n",
    "  - `dropout`: Dropout层的丢弃率。\n",
    "  - `norm`: 是否添加Batch Normalization层。\n",
    "  - `seed`: 随机种子，用于确保可重复性。\n",
    "  - **Encoder 部分**:\n",
    "    - 通过`MLP`类构建编码器，逐层将输入映射到潜在空间。\n",
    "  - **Decoder 部分**:\n",
    "    - 通过另一个`MLP`类构建解码器，将潜在空间映射回输入空间。\n",
    "\n",
    "- **`forward` 方法**:\n",
    "  - 将输入 `x` 传递通过编码器得到潜在表示 `rep`，然后通过解码器得到重构数据。\n",
    "  - 计算输入和重构数据之间的均方误差（MSE），作为损失函数。\n",
    "\n",
    "- **`get_latent_representation` 方法**:\n",
    "  - 给定输入 `x`，通过编码器和编码器到潜在空间的映射，返回潜在表示。\n",
    "\n",
    "- **`get_reconstruction` 方法**:\n",
    "  - 给定输入 `x`，通过整个自编码器的过程得到重构数据。\n",
    "\n",
    "- **`get_generative` 方法**:\n",
    "  - 给定潜在空间的表示 `z`，通过解码器生成相应的重构数据。\n",
    "\n",
    "### 代码的优缺点\n",
    "\n",
    "#### 优点\n",
    "- **模块化和可扩展性**: 代码通过 `MLP` 类封装了神经网络结构，使得可以在不同模型中复用。`AutoEncoder` 类则根据需要定义了编码器和解码器，便于修改和扩展。\n",
    "- **可配置性**: 在初始化时，用户可以灵活设置激活函数、隐藏层数、潜在空间的维度等参数，这增加了模型的灵活性。\n",
    "\n",
    "#### 缺点\n",
    "- **结构的复杂性**: 代码中存在一定的重复，例如编码器和解码器的结构非常相似，如果模型规模进一步增大，维护起来可能会变得困难。\n",
    "- **潜在空间模型的简化**: `LatentModel` 看起来是为变分自编码器（VAE）设计的，但并没有完全实现VAE的完整结构。具体而言，虽然计算了KL散度，但该类并没有包含生成部分，也没有显式地应用到主模型中，似乎是为了潜在空间的优化而存在，缺少对生成数据的明确操作。\n",
    "\n",
    "#### 为什么这个模型看起来没有使用VAE\n",
    "- 这段代码虽然定义了 `LatentModel` 来计算潜在空间的KL散度，但 `AutoEncoder` 类并没有采用该潜在模型来进行生成式建模。`AutoEncoder` 类的重点在于输入和输出之间的重构误差，而不是生成新数据。因此，虽然代码涉及潜在变量并计算KL散度，但没有将其与生成模型结合，缺少像VAE中通过重参数化技巧生成样本的部分。因此，这个模型可以看作是一个普通的自编码器，而不是一个完整的变分自编码器（VAE）。\n",
    "\n",
    "### 总结\n",
    "- 这个代码定义了一个标准的自编码器，其中涉及到潜在空间的操作，但没有完全实现VAE的生成过程。如果你打算扩展成VAE，应该进一步集成KL散度和生成过程到主模型的损失函数中。\n",
    "- 总体来说，代码逻辑清晰，结构可复用，但有进一步优化空间，尤其是在潜在空间模型的实现上，可以更好地与生成任务结合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "# from torchvision import datasets, transforms\n",
    "import collections\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from AE.utility import create_activation\n",
    "\n",
    "sys.path.append('../')\n",
    "def dataloader_split(X,test_size,seed,batch_size):\n",
    "    X_train, X_test = train_test_split(X, test_size=test_size, random_state=seed)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train,dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader,test_loader\n",
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 X,\n",
    "                 test_size:float=0.1,\n",
    "                 batch_size:int=32,\n",
    "                 lr:float=1e-3,\n",
    "                 weight_decay:float=0.0,\n",
    "                 seed:int=42,):\n",
    "        '''\n",
    "        Trainer for pretrain model.\n",
    "        Parameters:\n",
    "        model:\n",
    "            A pytorch model defined in \"models.py\"\n",
    "        X:\n",
    "            Feature matrix. mxn numpy array.\n",
    "                a standarized logorithmic data (i.e., zero mean, unit variance)\n",
    "        test_size:\n",
    "            fraction of testing/validation data size. default: 0.2\n",
    "        batch_size:\n",
    "            batch size.\n",
    "        lr:\n",
    "            learning rate.\n",
    "        weight_decay:\n",
    "            L2 regularization.\n",
    "        seed:\n",
    "            random seed.\n",
    "        '''\n",
    "        # self.args = args\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        # if self.model.decoder_type=='normal':\n",
    "        self.train_loader,self.test_loader=\\\n",
    "            dataloader_split(X,test_size,seed,batch_size)\n",
    "        self.seed=seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed(self.seed)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "    def train_step(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data,) in enumerate(self.train_loader):\n",
    "            data = data.to(self.device)\n",
    "            # scale_factor = scale_factor.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.model(data,)\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.item()#*data.shape[0]\n",
    "            self.optimizer.step()\n",
    "        train_loss=train_loss / len(self.train_loader.dataset)\n",
    "\n",
    "        return train_loss\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data,) in enumerate(self.test_loader):\n",
    "                data = data.to(self.device)\n",
    "                loss =self.model(data,)\n",
    "                test_loss += loss.item()#*data.shape[0]\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        return test_loss\n",
    "    def train(self, max_epoch=500,tol=1e-2,  patient=30):\n",
    "        # self.model.train()\n",
    "        self.model.history = {'train_loss': [], 'val_loss': [],\n",
    "                              'train_loss_ae':[],'val_loss_ae':[],\n",
    "                              'train_loss_topo':[],'val_loss_topo':[],\n",
    "                              'epoch':[]}\n",
    "        best_val_error = float('inf')\n",
    "        num_patient_epochs = 0\n",
    "        for epoch in range(max_epoch):\n",
    "            self.epoch=epoch\n",
    "            # Train for one epoch and get the training loss\n",
    "            train_loss = self.train_step()\n",
    "            # Compute the validation error\n",
    "            val_loss = self.test()\n",
    "            self.model.history['train_loss'].append(train_loss)\n",
    "            self.model.history['val_loss'].append(val_loss)\n",
    "            self.model.history['epoch'].append(epoch)\n",
    "            # if epoch % 10==0:\n",
    "            print(f\"Epoch {epoch}: train loss = {train_loss:.4f}, val error = {val_loss:.4f}\")\n",
    "            # Check if the validation error has decreased by at least tol\n",
    "            if best_val_error - val_loss >= tol:\n",
    "                best_val_error = val_loss\n",
    "                num_patient_epochs = 0\n",
    "            else:\n",
    "                num_patient_epochs += 1\n",
    "                # Check if we have exceeded the patience threshold\n",
    "            if num_patient_epochs >= patient:\n",
    "                print(f\"No improvement in validation error for {patient} epochs. Stopping early.\")\n",
    "                break\n",
    "        print(f\"Best validation error = {best_val_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个训练过程的管理类 `Trainer`，用于训练PyTorch模型，并包含了一些与训练相关的功能（如数据集拆分、训练过程、验证过程、早停机制等）。下面将逐行解释每个部分的功能和意义。\n",
    "\n",
    "### 1. **`dataloader_split` 函数**\n",
    "- **功能**: 将数据 `X` 划分为训练集和测试集，并创建相应的 `DataLoader`。\n",
    "- **参数**:\n",
    "  - `X`: 输入特征矩阵，形状为 `m x n`，即 `m` 个样本，`n` 个特征。\n",
    "  - `test_size`: 测试集的占比，默认是0.1（即90%的数据用于训练，10%的数据用于测试）。\n",
    "  - `seed`: 随机种子，用于确保划分的可重复性。\n",
    "  - `batch_size`: 每个批次的大小。\n",
    "- **实现**:\n",
    "  - 使用 `train_test_split` 将数据 `X` 随机分为训练集和测试集，比例由 `test_size` 参数决定。\n",
    "  - 将训练集和测试集转化为 `TensorDataset`，并用 `DataLoader` 包装，以便于后续批量处理。\n",
    "  - `DataLoader` 用于生成可迭代的数据批次。\n",
    "- **返回值**: `train_loader` 和 `test_loader`，分别是训练集和测试集的 `DataLoader`。\n",
    "\n",
    "### 2. **`Trainer` 类**\n",
    "- **功能**: 这个类用于管理模型的训练过程。它将模型的训练和验证过程封装在一起，方便训练的执行和管理。\n",
    "\n",
    "#### `__init__` 方法\n",
    "- **功能**: 初始化训练器。\n",
    "- **参数**:\n",
    "  - `model`: 要训练的模型（一个PyTorch模型对象）。\n",
    "  - `X`: 特征矩阵，类型为 `numpy` 数组。\n",
    "  - `test_size`: 测试集占比，默认为 `0.1`。\n",
    "  - `batch_size`: 批次大小，默认为 `32`。\n",
    "  - `lr`: 学习率，默认为 `1e-3`。\n",
    "  - `weight_decay`: L2正则化系数，默认为 `0.0`。\n",
    "  - `seed`: 随机种子，默认为 `42`。\n",
    "- **实现**:\n",
    "  - 根据是否有GPU，决定使用GPU（`cuda`）或CPU作为设备。\n",
    "  - 将模型加载到设备上（`model.to(self.device)`）。\n",
    "  - 使用 `dataloader_split` 函数分割数据 `X` 为训练集和测试集，并创建 `train_loader` 和 `test_loader`。\n",
    "  - 设置随机种子，保证实验可复现。\n",
    "  - 使用 `Adam` 优化器对模型的参数进行优化，学习率和L2正则化系数由外部传入。\n",
    "\n",
    "#### `train_step` 方法\n",
    "- **功能**: 执行一次训练步骤，即一次梯度更新。\n",
    "- **实现**:\n",
    "  - 将模型设置为训练模式 (`self.model.train()`)，这样会启用Dropout和BatchNorm等训练时特有的操作。\n",
    "  - 初始化 `train_loss` 变量，累积训练损失。\n",
    "  - 遍历 `train_loader` 中的每一个批次：\n",
    "    - 获取训练数据并将其移动到指定设备（GPU或CPU）。\n",
    "    - 清零优化器的梯度缓存。\n",
    "    - 计算模型的损失（通过调用模型的 `forward` 方法）。\n",
    "    - 反向传播计算梯度（`loss.backward()`）。\n",
    "    - 累积损失。\n",
    "    - 更新优化器的参数。\n",
    "  - 返回平均训练损失。\n",
    "\n",
    "#### `test` 方法\n",
    "- **功能**: 执行一次测试步骤，即计算验证集的损失。\n",
    "- **实现**:\n",
    "  - 将模型设置为评估模式 (`self.model.eval()`)，这样会禁用Dropout和BatchNorm等训练时特有的操作。\n",
    "  - 初始化 `test_loss` 变量，累积测试损失。\n",
    "  - 不计算梯度（`torch.no_grad()`），以节省计算资源。\n",
    "  - 遍历 `test_loader` 中的每一个批次：\n",
    "    - 获取测试数据并将其移动到指定设备（GPU或CPU）。\n",
    "    - 计算模型的损失。\n",
    "    - 累积损失。\n",
    "  - 返回平均测试损失。\n",
    "\n",
    "#### `train` 方法\n",
    "- **功能**: 训练模型，包含多个训练周期（Epochs），并实现了早停（Early Stopping）机制。\n",
    "- **参数**:\n",
    "  - `max_epoch`: 最大训练轮次，默认为 `500`。\n",
    "  - `tol`: 如果验证损失降低小于 `tol`，则认为模型没有进步，默认为 `1e-2`。\n",
    "  - `patient`: 如果验证损失连续若干轮没有改善，提前停止训练，默认为 `30`。\n",
    "- **实现**:\n",
    "  - 初始化历史记录字典 `self.model.history`，用来记录每一轮的训练和验证损失。\n",
    "  - 设置 `best_val_error` 为无穷大，用于记录验证集上的最佳损失。\n",
    "  - 设置 `num_patient_epochs` 用于记录当前没有进步的训练轮次。\n",
    "  - 循环 `max_epoch` 轮次进行训练：\n",
    "    - 调用 `train_step` 获取当前轮次的训练损失。\n",
    "    - 调用 `test` 获取当前轮次的验证损失。\n",
    "    - 将当前轮次的损失添加到历史记录中。\n",
    "    - 打印当前轮次的训练损失和验证损失。\n",
    "    - 检查验证损失是否比 `best_val_error` 小，如果减小则更新 `best_val_error`，并重置没有进步的计数。\n",
    "    - 如果连续 `patient` 轮次验证损失没有改善，则提前停止训练。\n",
    "  - 打印最终的最佳验证损失。\n",
    "\n",
    "### 总结\n",
    "这段代码实现了一个训练器类 `Trainer`，将模型训练、验证、早停等过程封装为一个类。它提供了以下功能：\n",
    "- 数据集的拆分和加载。\n",
    "- 每轮训练和验证的损失计算。\n",
    "- 训练过程中的早停机制。\n",
    "- 训练历史记录的保存。\n",
    "\n",
    "该类结构清晰，功能分明，适用于一般的机器学习模型训练过程，尤其适合于基于PyTorch框架的深度学习模型。它通过合理的抽象，帮助用户方便地管理和训练模型，同时还能灵活地设置训练的参数（如学习率、批次大小、训练轮数等）。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
