{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These codes come from TIGON  project (https://github.com/yutongo/TIGON/blob/19d6648195a47b4d2a2d5025b440d37cf0ac9a17/AE/models.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "# from torchvision import datasets, transforms\n",
    "import collections\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from AE.utility import create_activation,compute_distance_matrix\n",
    "sys.path.append('../')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers_list, dropout, norm,activation,last_act=False):\n",
    "        super(MLP, self).__init__()\n",
    "        layers=nn.ModuleList()\n",
    "        assert len(layers_list)>=2, 'no enough layers'\n",
    "        for i in range(len(layers_list)-2):\n",
    "            layers.append(nn.Linear(layers_list[i],layers_list[i+1]))\n",
    "            if norm:\n",
    "                layers.append(nn.BatchNorm1d(layers_list[i+1]))\n",
    "            if activation is not None:\n",
    "                layers.append(activation)\n",
    "            if dropout>0.:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(layers_list[-2],layers_list[-1]))\n",
    "        if norm:\n",
    "            layers.append(nn.BatchNorm1d(layers_list[-1]))\n",
    "        if last_act:\n",
    "            if activation is not None:\n",
    "                layers.append(activation)\n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        # layers.append(nn.Linear(layers_list[-1],out_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        # self.apply(init_weights_xavier_uniform)\n",
    "    def forward(self,x):\n",
    "        for layer in self.network:\n",
    "            x=layer(x)\n",
    "        return x\n",
    "class LatentModel(nn.Module):\n",
    "    def __init__(self,n_hidden,n_latent,\n",
    "                 kl_weight=1e-6, warmup_step=10000):\n",
    "        super(LatentModel,self).__init__()\n",
    "        self.mu = nn.Linear(n_hidden, n_latent)\n",
    "        self.logvar = nn.Linear(n_hidden, n_latent)\n",
    "        # self.kl = 0\n",
    "        self.kl_weight = kl_weight\n",
    "        self.step_count = 0\n",
    "        self.warmup_step = warmup_step\n",
    "\n",
    "    def kl_schedule_step(self):\n",
    "        self.step_count += 1\n",
    "        if self.step_count < self.warmup_step:\n",
    "            self.kl_weight = 0.0\n",
    "        else:\n",
    "            self.kl_weight = self.kl_weight + (1e-2 - 1e-6) / self.warmup_step\n",
    "\n",
    "        # elif self.step_count == self.warmup_step:\n",
    "        #     pass\n",
    "            # self.step_count = 0\n",
    "            # self.kl_weight = 1e-6\n",
    "\n",
    "    def forward(self, h):\n",
    "        mu = self.mu(h)\n",
    "        log_var = self.logvar(h)\n",
    "        sigma = torch.exp(0.5 * log_var)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        if self.training:\n",
    "            z = mu + sigma * epsilon\n",
    "            # (1 + log_var - mu ** 2 - log_var.exp()).sum()* self.kl_weight\n",
    "            # print('hhhhhhh')\n",
    "            self.kl = -0.5 * (1 + log_var - mu ** 2 - log_var.exp()).sum()  * self.kl_weight#/ z.shape[0]\n",
    "            self.kl_schedule_step()\n",
    "        else:\n",
    "            z = mu\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            n_layers: int = 1,\n",
    "            n_hidden: int = 500,\n",
    "            n_latent: int = 10,\n",
    "            activate_type: str='relu',\n",
    "            dropout: float = 0.2,\n",
    "            norm: bool = False,\n",
    "            seed: int=42,\n",
    "    ):\n",
    "        '''\n",
    "        Autoencoder model.\n",
    "        Encoder and Decoder take identical architectures.\n",
    "\n",
    "        Parameters:\n",
    "            in_dim:\n",
    "                dimension of the input feature\n",
    "            n_layers:\n",
    "                number of hidden layers\n",
    "            n_hidden:\n",
    "                dimension of hidden layer. All hidden layers take the same dimensions\n",
    "            n_latent:\n",
    "                dimension of latent space\n",
    "            activate_type:\n",
    "                activation functions.\n",
    "                Options: 'leakyrelu','relu', 'gelu', 'prelu', 'elu', and None for identity map.\n",
    "            dropout:\n",
    "                dropout rate\n",
    "            norm:\n",
    "                whether to include batch normalization layer\n",
    "            seed:\n",
    "                random seed.\n",
    "        '''\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        self.in_dim=in_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.n_hidden=n_hidden\n",
    "        self.n_latent=n_latent\n",
    "        self.dropout=dropout\n",
    "        self.norm=norm\n",
    "        self.activation=create_activation(activate_type)\n",
    "        ## Encoder:\n",
    "        self.encoder_layer=[in_dim]\n",
    "        for i in range(n_layers):\n",
    "            self.encoder_layer.append(n_hidden)\n",
    "        self.encoder=MLP(self.encoder_layer,dropout,norm,self.activation,last_act=True)\n",
    "        self.encoder_to_latent=MLP([self.encoder_layer[-1],n_latent],\n",
    "                                   dropout,norm,self.activation)\n",
    "\n",
    "        ## Decoder:\n",
    "        self.decoder_layer=[n_latent]\n",
    "        for i in range(n_layers):\n",
    "            self.decoder_layer.append(n_hidden)\n",
    "        self.decoder=MLP(self.decoder_layer,dropout,norm,self.activation,last_act=True)\n",
    "        self.decoder_to_output=MLP([self.decoder_layer[-1],self.in_dim],dropout,norm,activation=None)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        rep=self.get_latent_representation(x,tensor=True)\n",
    "        h = self.decoder(rep)\n",
    "        x_recon=self.decoder_to_output(h)\n",
    "        mse = nn.MSELoss(reduction='sum')\n",
    "        loss = mse(x_recon, x)/x.shape[1]\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def get_latent_representation(self,x,tensor:bool=False):\n",
    "        '''\n",
    "        Get latent space representation\n",
    "\n",
    "        Parameters\n",
    "        x:\n",
    "            Input space\n",
    "        tensor:\n",
    "            If input x is a tensor, or it is a numpy array\n",
    "        Return\n",
    "        rep:\n",
    "            latent space representation\n",
    "            If tensor==True:\n",
    "                return a tensor\n",
    "            If tensor==Flase:\n",
    "                return a numpy array\n",
    "        '''\n",
    "#        if not tensor:\n",
    "#            x=torch.tensor(x,dtype=torch.float32)\n",
    "#            self.eval()\n",
    "        x=self.encoder(x)\n",
    "        rep=self.encoder_to_latent(x)\n",
    "        #if not tensor:\n",
    "        #    rep=rep.detach().numpy()\n",
    "        return rep\n",
    "    def get_reconstruction(self, x):\n",
    "        '''\n",
    "        Reconstruct/impute gene expression data\n",
    "        x:\n",
    "            features. Numpy array\n",
    "        Return\n",
    "        x_recon:\n",
    "            Numpy array\n",
    "        '''\n",
    "        self.eval()\n",
    "        x=torch.tensor(x,dtype=torch.float32)\n",
    "#        with torch.no_grad():\n",
    "        x=self.encoder(x)\n",
    "        x=self.encoder_to_latent(x)\n",
    "        x = self.decoder(x)\n",
    "        x_recon = self.decoder_to_output(x)\n",
    "\n",
    "        #x_recon=x_recon.detach().numpy()\n",
    "        return x_recon\n",
    "    def get_generative(self,z):\n",
    "        '''\n",
    "        genereate gene expression data from latent space variable\n",
    "        z:\n",
    "            latent space representation. Numpy array\n",
    "        Return\n",
    "        x_recon:\n",
    "            Numpy array\n",
    "        '''\n",
    "        self.eval()\n",
    "        #z=torch.tensor(z,dtype=torch.float32)\n",
    "#        with torch.no_grad():\n",
    "        x = self.decoder(z)\n",
    "        x_recon = self.decoder_to_output(x)\n",
    "        #x_recon=x_recon.detach().numpy()\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个自编码器（Autoencoder, AE）和一些相关的模型架构，如 `MLP` 和 `LatentModel`。下面我将逐步解析每个函数的功能和具体实现。\n",
    "\n",
    "### 1. **`MLP` 类**\n",
    "- **功能**: 定义了一个多层感知机（MLP）模型。\n",
    "- **`__init__` 方法**:\n",
    "  - `layers_list`: 一个列表，定义了每一层的维度。\n",
    "  - `dropout`: Dropout层的丢弃率，用于防止过拟合。\n",
    "  - `norm`: 是否添加Batch Normalization层。\n",
    "  - `activation`: 激活函数（如ReLU, Sigmoid等）。\n",
    "  - `last_act`: 是否在最后一层添加激活函数。\n",
    "  - `self.network`: 将这些层和操作堆叠在一起，形成一个完整的神经网络。\n",
    "\n",
    "- **`forward` 方法**:\n",
    "  - 依次通过网络中的各层，返回网络的输出。\n",
    "\n",
    "### 2. **`LatentModel` 类**\n",
    "- **功能**: 定义了一个变分自编码器（VAE）中的潜在空间模型，生成潜在向量并计算KL散度（用于变分推断）。\n",
    "- **`__init__` 方法**:\n",
    "  - `n_hidden`: 隐藏层的维度。\n",
    "  - `n_latent`: 潜在空间的维度。\n",
    "  - `kl_weight`: KL散度的权重。\n",
    "  - `warmup_step`: 训练过程中KL散度的预热步数。\n",
    "\n",
    "- **`kl_schedule_step` 方法**:\n",
    "  - 用于逐步增加KL散度的权重，直到预热阶段结束。\n",
    "\n",
    "- **`forward` 方法**:\n",
    "  - 根据输入的隐藏层表示 `h`，计算潜在变量的均值 `mu` 和对数方差 `log_var`。\n",
    "  - 如果模型处于训练模式，生成潜在向量 `z` 并计算KL散度。\n",
    "  - 在推理模式下，直接使用均值 `mu` 作为潜在变量。\n",
    "\n",
    "### 3. **`AutoEncoder` 类**\n",
    "- **功能**: 定义了一个标准的自编码器（Autoencoder）。在训练过程中，通过最小化输入数据与重构数据之间的均方误差（MSE）来训练模型。\n",
    "- **`__init__` 方法**:\n",
    "  - `in_dim`: 输入数据的维度。\n",
    "  - `n_layers`: 隐藏层的层数。\n",
    "  - `n_hidden`: 隐藏层的维度。\n",
    "  - `n_latent`: 潜在空间的维度。\n",
    "  - `activate_type`: 激活函数类型（如ReLU, LeakyReLU等）。\n",
    "  - `dropout`: Dropout层的丢弃率。\n",
    "  - `norm`: 是否添加Batch Normalization层。\n",
    "  - `seed`: 随机种子，用于确保可重复性。\n",
    "  - **Encoder 部分**:\n",
    "    - 通过`MLP`类构建编码器，逐层将输入映射到潜在空间。\n",
    "  - **Decoder 部分**:\n",
    "    - 通过另一个`MLP`类构建解码器，将潜在空间映射回输入空间。\n",
    "\n",
    "- **`forward` 方法**:\n",
    "  - 将输入 `x` 传递通过编码器得到潜在表示 `rep`，然后通过解码器得到重构数据。\n",
    "  - 计算输入和重构数据之间的均方误差（MSE），作为损失函数。\n",
    "\n",
    "- **`get_latent_representation` 方法**:\n",
    "  - 给定输入 `x`，通过编码器和编码器到潜在空间的映射，返回潜在表示。\n",
    "\n",
    "- **`get_reconstruction` 方法**:\n",
    "  - 给定输入 `x`，通过整个自编码器的过程得到重构数据。\n",
    "\n",
    "- **`get_generative` 方法**:\n",
    "  - 给定潜在空间的表示 `z`，通过解码器生成相应的重构数据。\n",
    "\n",
    "### 代码的优缺点\n",
    "\n",
    "#### 优点\n",
    "- **模块化和可扩展性**: 代码通过 `MLP` 类封装了神经网络结构，使得可以在不同模型中复用。`AutoEncoder` 类则根据需要定义了编码器和解码器，便于修改和扩展。\n",
    "- **可配置性**: 在初始化时，用户可以灵活设置激活函数、隐藏层数、潜在空间的维度等参数，这增加了模型的灵活性。\n",
    "\n",
    "#### 缺点\n",
    "- **结构的复杂性**: 代码中存在一定的重复，例如编码器和解码器的结构非常相似，如果模型规模进一步增大，维护起来可能会变得困难。\n",
    "- **潜在空间模型的简化**: `LatentModel` 看起来是为变分自编码器（VAE）设计的，但并没有完全实现VAE的完整结构。具体而言，虽然计算了KL散度，但该类并没有包含生成部分，也没有显式地应用到主模型中，似乎是为了潜在空间的优化而存在，缺少对生成数据的明确操作。\n",
    "\n",
    "#### 为什么这个模型看起来没有使用VAE\n",
    "- 这段代码虽然定义了 `LatentModel` 来计算潜在空间的KL散度，但 `AutoEncoder` 类并没有采用该潜在模型来进行生成式建模。`AutoEncoder` 类的重点在于输入和输出之间的重构误差，而不是生成新数据。因此，虽然代码涉及潜在变量并计算KL散度，但没有将其与生成模型结合，缺少像VAE中通过重参数化技巧生成样本的部分。因此，这个模型可以看作是一个普通的自编码器，而不是一个完整的变分自编码器（VAE）。\n",
    "\n",
    "### 总结\n",
    "- 这个代码定义了一个标准的自编码器，其中涉及到潜在空间的操作，但没有完全实现VAE的生成过程。如果你打算扩展成VAE，应该进一步集成KL散度和生成过程到主模型的损失函数中。\n",
    "- 总体来说，代码逻辑清晰，结构可复用，但有进一步优化空间，尤其是在潜在空间模型的实现上，可以更好地与生成任务结合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
