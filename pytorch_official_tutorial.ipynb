{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_array: [[1 2]\n",
      " [3 4]],\n",
      " x_np:tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"np_array: {np_array},\\n x_np:{x_np}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_test = torch.from_numpy((np.array(np.arange(10)).reshape(2,5)))\n",
    "np_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.6961, 0.8659],\n",
      "        [0.4931, 0.4474]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.0970, 0.5097, 0.9500],\n",
      "        [0.0872, 0.7314, 0.3079]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 7],\n",
       "        [6, 3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(10,(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(torch.cuda.current_device())\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 已经转移到 cuda 上了\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor = tensor.to(device)\n",
    "print(f\"Tensor 已经转移到 {device} 上了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4,4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:,0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.stack([tensor,tensor,tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor,tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵乘法和逐元素乘法是两种不同的运算，在运算规则、形状要求、应用场景等方面都存在明显区别，下面为你详细介绍：\n",
    "\n",
    "### 运算规则\n",
    "- **矩阵乘法**：遵循线性代数中的矩阵乘法规则。若有两个矩阵 $\\mathbf{A}$ 和 $\\mathbf{B}$，设 $\\mathbf{A}$ 是 $m \\times n$ 的矩阵，$\\mathbf{B}$ 是 $n \\times p$ 的矩阵，那么它们的乘积 $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$ 是一个 $m \\times p$ 的矩阵。其中，$\\mathbf{C}$ 中第 $i$ 行第 $j$ 列的元素 $c_{ij}$ 等于 $\\mathbf{A}$ 的第 $i$ 行与 $\\mathbf{B}$ 的第 $j$ 列对应元素乘积之和，用公式表示为：\n",
    "\\[ c_{ij} = \\sum_{k = 1}^{n} a_{ik}b_{kj} \\]\n",
    "例如：\n",
    "\\[\n",
    "\\mathbf{A} = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}, \n",
    "\\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\\[\n",
    "\\mathbf{A} \\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "1\\times5 + 2\\times7 & 1\\times6 + 2\\times8 \\\\\n",
    "3\\times5 + 4\\times7 & 3\\times6 + 4\\times8\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "19 & 22 \\\\\n",
    "43 & 50\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "- **逐元素乘法**：也称为哈达玛积（Hadamard product），要求参与运算的两个矩阵形状相同。对于形状相同的矩阵 $\\mathbf{A}$ 和 $\\mathbf{B}$，它们的逐元素乘积 $\\mathbf{C} = \\mathbf{A} \\odot \\mathbf{B}$ 也是一个与它们形状相同的矩阵，其中 $\\mathbf{C}$ 中第 $i$ 行第 $j$ 列的元素 $c_{ij}$ 等于 $\\mathbf{A}$ 中第 $i$ 行第 $j$ 列的元素 $a_{ij}$ 与 $\\mathbf{B}$ 中第 $i$ 行第 $j$ 列的元素 $b_{ij}$ 直接相乘，即 $c_{ij} = a_{ij} \\times b_{ij}$。\n",
    "例如：\n",
    "\\[\n",
    "\\mathbf{A} = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}, \n",
    "\\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\\[\n",
    "\\mathbf{A} \\odot \\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "1\\times5 & 2\\times6 \\\\\n",
    "3\\times7 & 4\\times8\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "5 & 12 \\\\\n",
    "21 & 32\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "### 形状要求\n",
    "- **矩阵乘法**：前一个矩阵的列数必须等于后一个矩阵的行数。例如，一个 $3 \\times 2$ 的矩阵可以与一个 $2 \\times 4$ 的矩阵相乘，得到一个 $3 \\times 4$ 的矩阵，但不能与一个 $3 \\times 4$ 的矩阵相乘。\n",
    "- **逐元素乘法**：两个矩阵的形状必须完全相同。例如，两个 $3 \\times 2$ 的矩阵可以进行逐元素乘法，而一个 $3 \\times 2$ 的矩阵和一个 $2 \\times 3$ 的矩阵不能进行逐元素乘法。\n",
    "\n",
    "### 应用场景\n",
    "- **矩阵乘法**：\n",
    "    - **线性变换**：在计算机图形学中，矩阵乘法可用于实现图形的旋转、缩放、平移等线性变换。\n",
    "    - **神经网络**：在神经网络的全连接层中，输入层与隐藏层、隐藏层与输出层之间的连接通常通过矩阵乘法来实现。\n",
    "    - **数据降维**：主成分分析（PCA）等数据降维方法中也会用到矩阵乘法。\n",
    "- **逐元素乘法**：\n",
    "    - **激活函数**：在神经网络中，激活函数常常会对输入的每个元素进行逐元素操作，例如 ReLU 函数就是对输入的每个元素取最大值（0 和该元素本身）。\n",
    "    - **损失函数**：在一些损失函数中，会对预测值和真实值进行逐元素的加权操作，这时就会用到逐元素乘法。\n",
    "    - **特征缩放**：在数据预处理中，有时会对特征矩阵的每个元素进行缩放，也会用到逐元素乘法。\n",
    "\n",
    "综上所述，矩阵乘法和逐元素乘法是两种不同的运算，在实际应用中需要根据具体需求选择合适的运算方式。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t =torch.ones(5)\n",
    "n = t.numpy()\n",
    "print(t)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
