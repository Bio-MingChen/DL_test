{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data.shape)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.0105, 0.7984],\n",
      "        [0.9866, 0.3261]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_rand = torch.rand_like(x_data,dtype=torch.float)\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.0799, 0.9788, 0.0095],\n",
      "        [0.6659, 0.6016, 0.3410]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f'Shape of tensor: {tensor.shape}')\n",
    "print(f'Datatype of tensor: {tensor.dtype}')\n",
    "print(f'Device tensor is stored on: {tensor.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "\n",
    "tensor = torch.ones(4,4)\n",
    "print(f'First row: {tensor[0]}')\n",
    "print(f'First column: {tensor[:,0]}')\n",
    "print(f'Last column: {tensor[...,-1]}')\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.cat([tensor,tensor,tensor],dim=1)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor,tensor.T,out=y3)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵乘法并不意味着简单地将一个矩阵映射到另一个矩阵的形状上。矩阵乘法实际上是基于线性代数的定义，涉及到行和列之间的点乘计算。让我们详细讲解矩阵乘法的过程，特别是在本例中的计算方式。\n",
    "\n",
    "### 矩阵乘法的基本规则\n",
    "\n",
    "在矩阵乘法 \\( A @ B \\) 中，假设：\n",
    "- \\( A \\) 是一个形状为 \\( (m, n) \\) 的矩阵\n",
    "- \\( B \\) 是一个形状为 \\( (n, p) \\) 的矩阵\n",
    "\n",
    "那么，矩阵乘法 \\( A @ B \\) 的结果将是一个形状为 \\( (m, p) \\) 的矩阵。\n",
    "\n",
    "乘法的计算方式是：  \n",
    "**第 \\( i \\) 行和第 \\( j \\) 列的元素**是通过将 \\( A \\) 的第 \\( i \\) 行与 \\( B \\) 的第 \\( j \\) 列进行“点积”得到的。\n",
    "\n",
    "具体来说：\n",
    "\n",
    "\\[\n",
    "C[i, j] = \\sum_{k=1}^{n} A[i, k] \\times B[k, j]\n",
    "\\]\n",
    "\n",
    "其中，\\( A[i, k] \\) 是矩阵 \\( A \\) 中第 \\( i \\) 行，第 \\( k \\) 列的元素，\\( B[k, j] \\) 是矩阵 \\( B \\) 中第 \\( k \\) 行，第 \\( j \\) 列的元素，\\( C[i, j] \\) 是矩阵乘积 \\( C = A @ B \\) 中第 \\( i \\) 行，第 \\( j \\) 列的元素。\n",
    "\n",
    "### 本例中的矩阵乘法\n",
    "\n",
    "现在回到本例的代码：\n",
    "\n",
    "```python\n",
    "tensor = torch.tensor([[1., 0., 1., 1.],\n",
    "                       [1., 0., 1., 1.],\n",
    "                       [1., 0., 1., 1.],\n",
    "                       [1., 0., 1., 1.]])\n",
    "y1 = tensor @ tensor.T\n",
    "```\n",
    "\n",
    "在这个例子中，`tensor` 是一个 \\( 4 \\times 4 \\) 的矩阵，内容如下：\n",
    "\n",
    "\\[\n",
    "\\text{tensor} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "我们进行的是 `tensor` 与其转置 `tensor.T` 的矩阵乘法。\n",
    "\n",
    "`tensor.T` 的转置矩阵是：\n",
    "\n",
    "\\[\n",
    "\\text{tensor.T} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "矩阵乘法的计算过程如下：\n",
    "\n",
    "#### 第 1 行第 1 列\n",
    "\n",
    "计算 \\( y1[1,1] \\)：\n",
    "- `tensor` 的第 1 行是 \\( [1, 0, 1, 1] \\)\n",
    "- `tensor.T` 的第 1 列是 \\( [1, 1, 1, 1] \\)\n",
    "\n",
    "点乘的计算为：\n",
    "\n",
    "\\[\n",
    "y1[1, 1] = (1 \\times 1) + (0 \\times 1) + (1 \\times 1) + (1 \\times 1) = 3\n",
    "\\]\n",
    "\n",
    "#### 第 1 行第 2 列\n",
    "\n",
    "计算 \\( y1[1,2] \\)：\n",
    "- `tensor` 的第 1 行是 \\( [1, 0, 1, 1] \\)\n",
    "- `tensor.T` 的第 2 列是 \\( [0, 0, 0, 0] \\)\n",
    "\n",
    "点乘的计算为：\n",
    "\n",
    "\\[\n",
    "y1[1, 2] = (1 \\times 0) + (0 \\times 0) + (1 \\times 0) + (1 \\times 0) = 0\n",
    "\\]\n",
    "\n",
    "但是在这里，`tensor` 的每一行都是一样的，所以和 `tensor.T` 中的列的计算会得到相同的值。由于 `tensor` 的每行都是一样的，最终所有元素的计算结果都一样，都是 3。\n",
    "\n",
    "### 结果是一个对称矩阵\n",
    "\n",
    "由于 `tensor` 的每一行都相同，`tensor.T` 的每一列也相同，因此矩阵乘法的结果将是一个对称矩阵，其每个元素都为 3。矩阵乘法的结果如下：\n",
    "\n",
    "\\[\n",
    "y1 = \n",
    "\\begin{bmatrix}\n",
    "3 & 3 & 3 & 3 \\\\\n",
    "3 & 3 & 3 & 3 \\\\\n",
    "3 & 3 & 3 & 3 \\\\\n",
    "3 & 3 & 3 & 3\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "### 关键点总结：\n",
    "\n",
    "- **矩阵乘法**：是行与列之间的点乘操作。\n",
    "- 在本例中，`tensor` 的行和列都非常相似，导致了矩阵乘法结果的所有元素都相同。\n",
    "- `tensor @ tensor.T` 和 `tensor.matmul(tensor.T)` 的计算结果是一样的，因为它们本质上执行的是同样的矩阵乘法。\n",
    "- 本例的结果是一个 \\( 4 \\times 4 \\) 的矩阵，每个元素都是 3，因为所有的行和列之间的点乘结果都相同。\n",
    "\n",
    "所以，矩阵乘法的核心并不是形状映射，而是通过行列之间的点积计算出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 \\( X \\) 是一个 \\( 4 \\times 4 \\) 的矩阵，而 \\( Y \\) 是一个 \\( 4 \\times 2 \\) 的矩阵，矩阵乘法 \\( X @ Y \\) 是有效的，并且我们可以进行乘法运算。\n",
    "\n",
    "### 矩阵乘法的条件\n",
    "\n",
    "矩阵乘法的条件是：  \n",
    "**矩阵 \\( A \\) 和矩阵 \\( B \\) 相乘时，矩阵 \\( A \\) 的列数必须等于矩阵 \\( B \\) 的行数。**\n",
    "\n",
    "在本例中：\n",
    "- \\( X \\) 是 \\( 4 \\times 4 \\) 矩阵（4 行 4 列）。\n",
    "- \\( Y \\) 是 \\( 4 \\times 2 \\) 矩阵（4 行 2 列）。\n",
    "\n",
    "所以，矩阵乘法 \\( X @ Y \\) 是有效的，因为 \\( X \\) 的列数（4）等于 \\( Y \\) 的行数（4）。乘积矩阵 \\( Z = X @ Y \\) 的结果将会是一个 \\( 4 \\times 2 \\) 的矩阵。\n",
    "\n",
    "### 矩阵乘法的计算\n",
    "\n",
    "假设：\n",
    "- \\( X \\) 的形状为 \\( 4 \\times 4 \\)\n",
    "- \\( Y \\) 的形状为 \\( 4 \\times 2 \\)\n",
    "\n",
    "结果矩阵 \\( Z \\) 的形状将是 \\( 4 \\times 2 \\)。\n",
    "\n",
    "#### 计算 \\( Z \\) 中的每个元素\n",
    "\n",
    "矩阵乘法的计算方式是行与列之间的点积。对于每个元素 \\( Z[i, j] \\)，我们需要计算 \\( X \\) 的第 \\( i \\) 行和 \\( Y \\) 的第 \\( j \\) 列的点积：\n",
    "\n",
    "\\[\n",
    "Z[i, j] = \\sum_{k=1}^{4} X[i, k] \\times Y[k, j]\n",
    "\\]\n",
    "\n",
    "也就是说，结果矩阵 \\( Z \\) 中的第 \\( i \\) 行第 \\( j \\) 列的元素是 \\( X \\) 的第 \\( i \\) 行与 \\( Y \\) 的第 \\( j \\) 列相对应元素的乘积之和。\n",
    "\n",
    "### 例子\n",
    "\n",
    "假设有以下矩阵 \\( X \\) 和 \\( Y \\)：\n",
    "\n",
    "\\[\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 10 & 11 & 12 \\\\\n",
    "13 & 14 & 15 & 16\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "#### 计算 \\( Z = X @ Y \\)\n",
    "\n",
    "1. **第 1 行第 1 列的元素：**\n",
    "\n",
    "   \\[\n",
    "   Z[1, 1] = (1 \\times 1) + (2 \\times 3) + (3 \\times 5) + (4 \\times 7) = 1 + 6 + 15 + 28 = 50\n",
    "   \\]\n",
    "\n",
    "2. **第 1 行第 2 列的元素：**\n",
    "\n",
    "   \\[\n",
    "   Z[1, 2] = (1 \\times 2) + (2 \\times 4) + (3 \\times 6) + (4 \\times 8) = 2 + 8 + 18 + 32 = 60\n",
    "   \\]\n",
    "\n",
    "3. **第 2 行第 1 列的元素：**\n",
    "\n",
    "   \\[\n",
    "   Z[2, 1] = (5 \\times 1) + (6 \\times 3) + (7 \\times 5) + (8 \\times 7) = 5 + 18 + 35 + 56 = 114\n",
    "   \\]\n",
    "\n",
    "4. **第 2 行第 2 列的元素：**\n",
    "\n",
    "   \\[\n",
    "   Z[2, 2] = (5 \\times 2) + (6 \\times 4) + (7 \\times 6) + (8 \\times 8) = 10 + 24 + 42 + 64 = 140\n",
    "   \\]\n",
    "\n",
    "类似地，计算 \\( Z[3, 1] \\), \\( Z[3, 2] \\), \\( Z[4, 1] \\), 和 \\( Z[4, 2] \\)。\n",
    "\n",
    "最终，矩阵 \\( Z \\) 会是：\n",
    "\n",
    "\\[\n",
    "Z = \n",
    "\\begin{bmatrix}\n",
    "50 & 60 \\\\\n",
    "114 & 140 \\\\\n",
    "178 & 220 \\\\\n",
    "242 & 300\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "### 总结\n",
    "\n",
    "- 矩阵 \\( X \\) 的形状是 \\( 4 \\times 4 \\)，矩阵 \\( Y \\) 的形状是 \\( 4 \\times 2 \\)，它们的乘积 \\( Z = X @ Y \\) 的形状是 \\( 4 \\times 2 \\)。\n",
    "- 计算时，乘积矩阵 \\( Z \\) 的每个元素是 \\( X \\) 的某一行与 \\( Y \\) 的某一列的点积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1403, 0.6154, 0.8966, 0.6262, 0.3111, 0.6699],\n",
      "        [0.7341, 0.7049, 0.6820, 0.1914, 0.8069, 0.6850],\n",
      "        [0.2548, 0.5455, 0.2311, 0.9859, 0.1084, 0.7951],\n",
      "        [0.2052, 0.6623, 0.6246, 0.7815, 0.9876, 0.5249],\n",
      "        [0.3920, 0.5089, 0.2981, 0.6649, 0.0459, 0.2363],\n",
      "        [0.8587, 0.9652, 0.9240, 0.0613, 0.4841, 0.7637]])\n",
      "tensor([[0.5279, 0.6723, 0.6178, 0.2747, 0.1119, 0.2743],\n",
      "        [0.1172, 0.1397, 0.0352, 0.0479, 0.5422, 0.4105],\n",
      "        [0.9544, 0.1355, 0.0083, 0.2314, 0.3468, 0.3941],\n",
      "        [0.5486, 0.5482, 0.1439, 0.6531, 0.8854, 0.3003],\n",
      "        [0.6828, 0.1225, 0.7647, 0.6213, 0.8857, 0.7146],\n",
      "        [0.3945, 0.4918, 0.9230, 0.1345, 0.2458, 0.1519]])\n"
     ]
    }
   ],
   "source": [
    "z_rand = torch.rand(6,6)\n",
    "print(z_rand)\n",
    "z_copy = torch.rand_like(z_rand)\n",
    "print(z_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor,tensor,out=z3)\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.) 12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# get numerical value of tensor by item()\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg,agg_item,type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f'{tensor} \\n')\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f't: {t}')\n",
    "n = t.numpy()\n",
    "print(f'n: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1) # in place operation\n",
    "print(f't: {t}')\n",
    "print(f'n: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape: torch.Size([2, 2]), b shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2,dtype=torch.int32)\n",
    "b = torch.tensor([[1,2,3],[1,2,3]],dtype=torch.int32)\n",
    "print(f\"a shape: {a.shape}, b shape: {b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3]], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 6],\n",
       "        [2, 4, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不完全是这样的，矩阵乘法的计算规则稍微复杂一些。我们来详细解释一下。\n",
    "\n",
    "### **矩阵乘法本质：**\n",
    "矩阵乘法的本质是**\"行乘列\"**，而不是**\"元素逐个相乘\"**。\n",
    "\n",
    "#### **行乘列规则：**\n",
    "- 如果有两个矩阵 \\(A\\) 和 \\(B\\)，其中：\n",
    "  - \\(A\\) 的形状是 \\((m, n)\\)\n",
    "  - \\(B\\) 的形状是 \\((n, p)\\)\n",
    "\n",
    "- 矩阵乘法 \\(C = A @ B\\) 的结果矩阵 \\(C\\) 的形状是 \\((m, p)\\)。\n",
    "\n",
    "- **第 \\(i, j\\) 个元素的计算公式为：**\n",
    "  \\[\n",
    "  C_{ij} = \\sum_{k=1}^{n} A_{ik} \\times B_{kj}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **结合你的例子：**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.ones(2, 2, dtype=torch.int32)  # 2x2 矩阵\n",
    "b = torch.tensor([[1, 2, 3], [1, 2, 3]], dtype=torch.int32)  # 2x3 矩阵\n",
    "\n",
    "c = torch.matmul(a, b)  # 矩阵乘法\n",
    "print(c)\n",
    "```\n",
    "\n",
    "#### **矩阵 \\(A\\) 和 \\(B\\)：**\n",
    "\\[\n",
    "A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### **计算第一个元素 \\(C_{00}\\)：**\n",
    "取 **A 的第 1 行** 和 **B 的第 1 列**：\n",
    "\\[\n",
    "C_{00} = (1 \\times 1) + (1 \\times 1) = 2\n",
    "\\]\n",
    "\n",
    "#### **计算第二个元素 \\(C_{01}\\)：**\n",
    "取 **A 的第 1 行** 和 **B 的第 2 列**：\n",
    "\\[\n",
    "C_{01} = (1 \\times 2) + (1 \\times 2) = 4\n",
    "\\]\n",
    "\n",
    "#### **计算第三个元素 \\(C_{02}\\)：**\n",
    "取 **A 的第 1 行** 和 **B 的第 3 列**：\n",
    "\\[\n",
    "C_{02} = (1 \\times 3) + (1 \\times 3) = 6\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### **第二行元素计算（同样的逻辑）：**\n",
    "由于 \\(A\\) 的第二行和第一行相同，因此计算结果相同：\n",
    "\\[\n",
    "C_{10} = (1 \\times 1) + (1 \\times 1) = 2\n",
    "\\]\n",
    "\\[\n",
    "C_{11} = (1 \\times 2) + (1 \\times 2) = 4\n",
    "\\]\n",
    "\\[\n",
    "C_{12} = (1 \\times 3) + (1 \\times 3) = 6\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### **最终结果矩阵：**\n",
    "\\[\n",
    "C = \\begin{bmatrix} 2 & 4 & 6 \\\\ 2 & 4 & 6 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **关键点：**\n",
    "1. **行乘列**：并不是用 \\(A\\) 的每个元素去乘 \\(B\\) 的每一列，而是用 \\(A\\) 的**整行**去乘 \\(B\\) 的**整列**，对应元素相乘后**求和**。\n",
    "2. **相乘顺序**：对于每个位置 \\((i, j)\\)，遍历 \\(A\\) 的第 \\(i\\) 行和 \\(B\\) 的第 \\(j\\) 列，对应元素乘积**求和**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0],\n",
       "        [0, 2]], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([[2,0],[0,2]],dtype=torch.int32)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 6],\n",
       "        [2, 4, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 6],\n",
       "        [2, 4, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0],\n",
       "        [0, 2]], dtype=torch.int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.tensor([[1,0,0],[0,1,0]],dtype=torch.int32)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3]], dtype=torch.int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [2, 2]], dtype=torch.int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d @ b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"check_uniform_bounds\" not implemented for 'Int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: \"check_uniform_bounds\" not implemented for 'Int'"
     ]
    }
   ],
   "source": [
    "torch.rand_like(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: 1D tensors expected, but got 2D and 2D tensors"
     ]
    }
   ],
   "source": [
    "torch.dot(a,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，我们来详细计算以下矩阵乘法：\n",
    "\n",
    "### **1. 矩阵定义：**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "d = torch.tensor([[1, 0, 0], [0, 1, 0]], dtype=torch.int32)\n",
    "b = torch.tensor([[1, 2, 3], [1, 2, 3]], dtype=torch.int32)\n",
    "\n",
    "print(f\"d shape: {d.shape}, b shape: {b.shape}\")\n",
    "```\n",
    "\n",
    "输出：\n",
    "```\n",
    "d shape: torch.Size([2, 3]), b shape: torch.Size([2, 3])\n",
    "```\n",
    "\n",
    "#### **矩阵 d：** (2x3)\n",
    "\\[\n",
    "d = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "#### **矩阵 b：** (2x3)\n",
    "\\[\n",
    "b = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 乘法规则：**\n",
    "矩阵乘法要求：\n",
    "- **d 的列数**（3）要等于 **b 的行数**（2）。  \n",
    "- 因为不符合矩阵乘法规则，所以**直接计算 \\(d \\times b\\) 会报错**。\n",
    "\n",
    "**矩阵乘法的维度要求：**\n",
    "\\[\n",
    "(m, n) \\times (n, p) \\implies (m, p)\n",
    "\\]\n",
    "- 但是，这里 **\\(d\\) 是 \\(2 \\times 3\\)**，**\\(b\\) 也是 \\(2 \\times 3\\)**，它们**不符合矩阵乘法要求**。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 如何理解这个问题？**\n",
    "矩阵乘法要求：\n",
    "- **第一个矩阵的列数** = **第二个矩阵的行数**。  \n",
    "- 由于 \\(d\\) 和 \\(b\\) 都是 \\(2 \\times 3\\)，它们无法直接相乘。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 是否可以换乘法顺序？**\n",
    "如果我们尝试计算 \\(b \\times d^T\\)（即将 d 转置），则：\n",
    "- \\(d^T\\) 的形状为 \\(3 \\times 2\\)，因此：\n",
    "\\[\n",
    "b \\times d^T \\implies (2 \\times 3) \\times (3 \\times 2) \\implies (2 \\times 2)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 计算 \\(b \\times d^T\\)：**\n",
    "\\[\n",
    "d^T = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "b \\times d^T = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix} \\times \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "#### **计算第一个元素 \\(C_{00}\\)：**\n",
    "\\[\n",
    "C_{00} = (1 \\times 1) + (2 \\times 0) + (3 \\times 0) = 1\n",
    "\\]\n",
    "\n",
    "#### **计算第二个元素 \\(C_{01}\\)：**\n",
    "\\[\n",
    "C_{01} = (1 \\times 0) + (2 \\times 1) + (3 \\times 0) = 2\n",
    "\\]\n",
    "\n",
    "#### **计算第三个元素 \\(C_{10}\\)：**\n",
    "\\[\n",
    "C_{10} = (1 \\times 1) + (2 \\times 0) + (3 \\times 0) = 1\n",
    "\\]\n",
    "\n",
    "#### **计算第四个元素 \\(C_{11}\\)：**\n",
    "\\[\n",
    "C_{11} = (1 \\times 0) + (2 \\times 1) + (3 \\times 0) = 2\n",
    "\\]\n",
    "\n",
    "#### **最终结果矩阵：**\n",
    "\\[\n",
    "C = \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Python 验证：**\n",
    "```python\n",
    "d_t = d.T  # 转置矩阵 d\n",
    "c = torch.matmul(b, d_t)  # 矩阵乘法\n",
    "print(\"Matrix C:\\n\", c)\n",
    "```\n",
    "\n",
    "#### **输出：**\n",
    "```\n",
    "Matrix C:\n",
    " tensor([[1, 2],\n",
    "         [1, 2]], dtype=torch.int32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. 解释：**\n",
    "- **d 矩阵**在这里表现为一个**选择矩阵**，它**提取了 b 矩阵的第一列和第二列**，而忽略了第三列。  \n",
    "- **变换意义：**  \n",
    "  - **d** 矩阵相当于对 b 进行**维度压缩**，只提取特定方向上的分量。  \n",
    "  - **换句话说：** \\(d\\) 就是一个**线性投影矩阵**，将 3 维向量压缩到 2 维。\n",
    "\n",
    "---\n",
    "\n",
    "### **8. 几何意义：**\n",
    "- 假如 \\(b\\) 矩阵中的每一列代表一个**三维空间向量**，那么 \\(d^T\\) 相当于将这些向量**投影到二维平面**。  \n",
    "- 结果矩阵 \\(C\\) 中的每个元素都是**输入向量在二维空间中的投影值**。\n",
    "\n",
    "---\n",
    "\n",
    "### **总结：**\n",
    "- **矩阵乘法顺序很重要：** 在不能直接相乘的情况下，尝试**转置操作**。  \n",
    "- **线性投影本质：** 这里，\\(d\\) 作为投影矩阵，提取了 \\(b\\) 中每个向量的特定维度。  \n",
    "- **应用场景：** 这种投影在**降维、特征提取**和**线性变换**中非常常见。  \n",
    "\n",
    "如果有进一步的问题，随时交流！😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不，**点积**不仅仅可以用于两个一维向量（1D 向量），它还可以在更高维度的矩阵中使用，但其含义和计算方式有所不同。\n",
    "\n",
    "### **一、点积的常见使用场景：**\n",
    "1. **两个一维向量（1D 向量）之间：**  \n",
    "   \\[\n",
    "   \\mathbf{a} \\cdot \\mathbf{b} = \\sum_i a_i \\times b_i\n",
    "   \\]\n",
    "   - 这是最经典的点积形式，也是我们通常所指的**点积**。\n",
    "\n",
    "2. **矩阵的行和列（矩阵乘法的一个元素）：**  \n",
    "   如果有两个矩阵 \\(A\\) 和 \\(B\\)，矩阵乘法 \\(C = A \\times B\\) 的元素 \\(C_{ij}\\) 就是**A 的第 \\(i\\) 行与 B 的第 \\(j\\) 列的点积**：\n",
    "   \\[\n",
    "   C_{ij} = \\mathbf{a}_i \\cdot \\mathbf{b}_j\n",
    "   \\]\n",
    "\n",
    "3. **批量向量点积（批处理）：**  \n",
    "   当有多个向量对需要同时计算点积时，可以利用**广播机制**或**批量点积**操作。\n",
    "\n",
    "---\n",
    "\n",
    "### **二、点积在 PyTorch 中的应用：**\n",
    "\n",
    "#### **1. 一维向量之间的点积：**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "b = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "\n",
    "# 点积\n",
    "dot_product = torch.dot(a, b)\n",
    "print(f\"点积（1D向量）：{dot_product}\")\n",
    "```\n",
    "**输出：**\n",
    "```\n",
    "点积（1D向量）：32.0\n",
    "```\n",
    "**解释：**\n",
    "\\[\n",
    "1 \\times 4 + 2 \\times 5 + 3 \\times 6 = 32\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. 矩阵之间的点积（矩阵乘法中的单元点积）：**\n",
    "假设：\n",
    "\\[\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n",
    "\\]\n",
    "**矩阵乘法：**\n",
    "\\[\n",
    "C = A \\times B\n",
    "\\]\n",
    "计算：\n",
    "\\[\n",
    "C_{00} = (1 \\times 5) + (2 \\times 7) = 19\n",
    "\\]\n",
    "\\[\n",
    "C_{01} = (1 \\times 6) + (2 \\times 8) = 22\n",
    "\\]\n",
    "\n",
    "**代码：**\n",
    "```python\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# 矩阵乘法\n",
    "C = torch.matmul(A, B)\n",
    "print(\"矩阵乘法结果：\\n\", C)\n",
    "```\n",
    "**输出：**\n",
    "```\n",
    "矩阵乘法结果：\n",
    " tensor([[19., 22.],\n",
    "         [43., 50.]])\n",
    "```\n",
    "**解释：**\n",
    "- 每个元素 \\(C_{ij}\\) 是 \\(A\\) 第 \\(i\\) 行与 \\(B\\) 第 \\(j\\) 列的点积。\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. 批量点积（3D Tensor点积）：**\n",
    "假设我们有批量数据：\n",
    "```python\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)  # 2x3\n",
    "b = torch.tensor([[7, 8, 9], [10, 11, 12]], dtype=torch.float32)  # 2x3\n",
    "\n",
    "# 批量点积\n",
    "batch_dot_product = torch.bmm(a.unsqueeze(1), b.unsqueeze(2)).squeeze()\n",
    "print(f\"批量点积：{batch_dot_product}\")\n",
    "```\n",
    "**输出：**\n",
    "```\n",
    "批量点积：tensor([ 50., 154.])\n",
    "```\n",
    "**解释：**\n",
    "\\[\n",
    "(1 \\times 7) + (2 \\times 8) + (3 \\times 9) = 50\n",
    "\\]\n",
    "\\[\n",
    "(4 \\times 10) + (5 \\times 11) + (6 \\times 12) = 154\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. 多维张量点积（内积）：**\n",
    "当处理高维张量时，比如**3D 张量**（批量、特征、嵌入）：\n",
    "```python\n",
    "# 三维张量：批量大小为 2，特征为 3\n",
    "x = torch.randn(2, 3, 4)  # (batch, feature, embedding)\n",
    "y = torch.randn(2, 4, 3)  # (batch, embedding, feature)\n",
    "\n",
    "# 批量点积\n",
    "result = torch.matmul(x, y)  # (2, 3, 3)\n",
    "print(\"批量矩阵乘法：\\n\", result.shape)\n",
    "```\n",
    "**输出：**\n",
    "```\n",
    "批量矩阵乘法：\n",
    " torch.Size([2, 3, 3])\n",
    "```\n",
    "**解释：**\n",
    "- 每个批次中的**点积**是相应维度上进行的，类似于批量矩阵乘法。\n",
    "\n",
    "---\n",
    "\n",
    "### **三、点积与矩阵乘法的区别：**\n",
    "\n",
    "| 点积                   | 矩阵乘法                               |\n",
    "|-----------------------|-----------------------------------------|\n",
    "| 只作用于**1D 向量**       | 作用于**2D 或更高维度矩阵**             |\n",
    "| 结果是一个**标量**       | 结果是一个**矩阵**                     |\n",
    "| 计算方式：**元素对应相乘求和** | 计算方式：**行与列的点积，逐元素求和** |\n",
    "\n",
    "---\n",
    "\n",
    "### **四、总结：**\n",
    "1. **点积不局限于1D向量**，在矩阵计算中也会频繁出现，如**矩阵乘法中的单元计算**。  \n",
    "2. **批量点积**可以通过**张量广播**和**批矩阵乘法（bmm）**来实现。  \n",
    "3. **矩阵乘法与点积的关系：** 矩阵乘法的每个单元本质上是**行和列的点积**。  \n",
    "4. **场景：** 从简单的**向量相似度计算**到复杂的**神经网络权重计算**，点积无处不在。  \n",
    "\n",
    "如果还有什么困惑或者想了解更多实际应用，随时问我！😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，**两个矩阵直接相乘（使用 `*` 或 `torch.mul()`）**，表示的是**逐元素相乘（Element-wise Multiplication）**，而不是矩阵乘法或点积。\n",
    "\n",
    "---\n",
    "\n",
    "### **一、三种乘法的对比**\n",
    "\n",
    "| 操作        | 计算符号          | 含义                       | 结果形状                          | 典型应用                  |\n",
    "|------------|-----------------|----------------------------|----------------------------------|--------------------------|\n",
    "| 点积        | `torch.dot()`    | 两个向量对应元素乘积求和     | 标量 (scalar)                     | 向量相似性、内积计算      |\n",
    "| 矩阵乘法    | `@` 或 `torch.matmul()` | 矩阵的线性组合和投影        | \\((m, p)\\)，如果形状为 \\((m, n)\\) 和 \\((n, p)\\) | 线性变换、旋转、投影      |\n",
    "| 元素乘法    | `*` 或 `torch.mul()`    | 元素对应位置相乘             | 相同形状                         | 哈达玛积（Hadamard Product）、逐元素操作 |\n",
    "\n",
    "---\n",
    "\n",
    "### **二、三种乘法的数学表达**\n",
    "\n",
    "#### **1. 点积（Dot Product）:**\n",
    "点积仅用于**两个相同长度的一维向量**：\n",
    "\\[\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i \\times b_i\n",
    "\\]\n",
    "**Python 示例：**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "b = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "\n",
    "dot_product = torch.dot(a, b)\n",
    "print(f\"点积: {dot_product}\")\n",
    "```\n",
    "**输出：**\n",
    "```\n",
    "点积: 32.0\n",
    "```\n",
    "**计算：** \\(1 \\times 4 + 2 \\times 5 + 3 \\times 6 = 32\\)\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. 矩阵乘法（Matrix Multiplication）：**\n",
    "用于**二维及以上矩阵**，需要满足形状匹配：  \n",
    "\\[\n",
    "\\mathbf{C} = \\mathbf{A} @ \\mathbf{B}\n",
    "\\]\n",
    "\\[\n",
    "C_{ij} = \\sum_{k=1}^{n} A_{ik} \\times B_{kj}\n",
    "\\]\n",
    "**Python 示例：**\n",
    "```python\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "matrix_product = torch.matmul(A, B)\n",
    "print(\"矩阵乘法结果:\\n\", matrix_product)\n",
    "```\n",
    "**输出：**\n",
    "```\n",
    "矩阵乘法结果:\n",
    " tensor([[19., 22.],\n",
    "         [43., 50.]])\n",
    "```\n",
    "**计算：**  \n",
    "\\[\n",
    "C_{00} = (1 \\times 5) + (2 \\times 7) = 19\n",
    "\\]  \n",
    "\\[\n",
    "C_{01} = (1 \\times 6) + (2 \\times 8) = 22\n",
    "\\]  \n",
    "\\[\n",
    "C_{10} = (3 \\times 5) + (4 \\times 7) = 43\n",
    "\\]  \n",
    "\\[\n",
    "C_{11} = (3 \\times 6) + (4 \\times 8) = 50\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. 元素乘法（Element-wise Multiplication）：**\n",
    "直接使用 `*` 或 `torch.mul()`，要求**两个矩阵形状相同**：\n",
    "\\[\n",
    "\\mathbf{C} = \\mathbf{A} * \\mathbf{B}\n",
    "\\]\n",
    "**Python 示例：**\n",
    "```python\n",
    "elementwise_product = A * B\n",
    "print(\"逐元素乘法结果:\\n\", elementwise_product)\n",
    "```\n",
    "**输出：**\n",
    "```\n",
    "逐元素乘法结果:\n",
    " tensor([[ 5., 12.],\n",
    "         [21., 32.]])\n",
    "```\n",
    "**计算：**  \n",
    "\\[\n",
    "C_{00} = 1 \\times 5 = 5\n",
    "\\]  \n",
    "\\[\n",
    "C_{01} = 2 \\times 6 = 12\n",
    "\\]  \n",
    "\\[\n",
    "C_{10} = 3 \\times 7 = 21\n",
    "\\]  \n",
    "\\[\n",
    "C_{11} = 4 \\times 8 = 32\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **三、元素乘法的应用场景**\n",
    "\n",
    "1. **哈达玛积（Hadamard Product）：**  \n",
    "   - 用于**逐元素权重化**或**掩膜操作**。  \n",
    "   - 在**卷积神经网络（CNN）**中，逐元素乘法用于**门控机制**。  \n",
    "\n",
    "2. **损失计算：**  \n",
    "   - 在损失函数中，如**均方误差（MSE）**，逐元素求平方：  \n",
    "     \\[\n",
    "     \\text{MSE} = \\frac{1}{n} \\sum (y_{true} - y_{pred})^2\n",
    "     \\]\n",
    "\n",
    "3. **特征交互：**  \n",
    "   - 在**推荐系统**中，可以将两个特征向量逐元素相乘，得到**特征交互值**。  \n",
    "\n",
    "4. **逐元素操作：**  \n",
    "   - 在图神经网络（GNN）中，有时需要将节点特征进行逐元素加权（注意力机制）。  \n",
    "\n",
    "---\n",
    "\n",
    "### **四、点积、矩阵乘法、元素乘法的核心区别：**\n",
    "\n",
    "| 特性                   | 点积                 | 矩阵乘法                    | 元素乘法                      |\n",
    "|-----------------------|-----------------------|-----------------------------|-------------------------------|\n",
    "| 输入要求               | 一维向量               | 二维或更高维矩阵             | 相同形状的矩阵或张量           |\n",
    "| 输出形式               | 标量（scalar）          | 矩阵                        | 矩阵                          |\n",
    "| 计算方式               | 对应元素相乘再求和       | 行与列相乘并累加              | 对应元素直接相乘               |\n",
    "| 几何意义               | 两个向量的投影和夹角关系 | 线性变换或旋转                | 元素间逐个乘积，逐元素变换      |\n",
    "| 典型应用               | 向量相似性、能量计算     | 线性变换、特征映射            | 权重加权、掩膜操作、逐元素计算 |\n",
    "\n",
    "---\n",
    "\n",
    "### **五、总结：**\n",
    "1. **点积**适用于**向量相似性计算**，结果是一个**标量**，反映两个向量在相同方向上的投影。  \n",
    "2. **矩阵乘法**用于**线性变换和空间映射**，是神经网络中计算**加权和**的核心操作。  \n",
    "3. **逐元素乘法**是**按位计算**，对**相同形状的矩阵**逐元素操作，应用于**权重化、特征交互、哈达玛积**等。  \n",
    "4. **计算成本：**  \n",
    "   - 点积：\\(O(n)\\)  \n",
    "   - 矩阵乘法：\\(O(m \\times n \\times p)\\)  \n",
    "   - 元素乘法：\\(O(m \\times n)\\)  \n",
    "\n",
    "如果还有疑问或需要进一步示例，请随时联系我！😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matrix multiplication\n",
    "- element-wise product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数学和日常用语中，**\"multiplication\"** 和 **\"product\"** 都与“乘法”相关，但它们的含义和使用场景有所不同。以下是它们的区别：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Multiplication（乘法）**\n",
    "- **词性：** 名词\n",
    "- **含义：** 一种数学运算，表示通过将一个数与另一个数相乘来得到结果。\n",
    "- **用法：** 多用于描述**运算过程**。\n",
    "- **中文翻译：** 乘法、相乘\n",
    "\n",
    "#### **示例：**\n",
    "- **The multiplication of 3 and 4 is written as 3 × 4.**  \n",
    "  （3 和 4 的乘法写作 3 × 4。）\n",
    "\n",
    "- **Multiplication is one of the four basic operations in arithmetic.**  \n",
    "  （乘法是算术的四种基本运算之一。）\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Product（乘积）**\n",
    "- **词性：** 名词\n",
    "- **含义：** 乘法运算后的**结果**。\n",
    "- **用法：** 多用于描述**运算结果**。\n",
    "- **中文翻译：** 乘积\n",
    "\n",
    "#### **示例：**\n",
    "- **The product of 3 and 4 is 12.**  \n",
    "  （3 和 4 的乘积是 12。）\n",
    "\n",
    "- **Finding the product of two numbers involves multiplication.**  \n",
    "  （求两个数的乘积涉及乘法运算。）\n",
    "\n",
    "---\n",
    "\n",
    "### **总结：**\n",
    "| 区别        | Multiplication                 | Product            |\n",
    "|------------|--------------------------------|---------------------|\n",
    "| **含义**     | 运算过程，表示两个数相乘         | 运算结果，表示乘法的结果 |\n",
    "| **词性**     | 名词（表示过程）                 | 名词（表示结果）       |\n",
    "| **中文翻译** | 乘法、相乘                       | 乘积                 |\n",
    "| **示例**     | The multiplication of 3 and 4. | The product of 3 and 4. |\n",
    "\n",
    "---\n",
    "\n",
    "### **一个比喻：**\n",
    "- **Multiplication** 就像**烹饪过程**，表示把食材（数）混合在一起。  \n",
    "- **Product** 就像**成品菜**，是烹饪完成后得到的结果。\n",
    "\n",
    "---\n",
    "\n",
    "在实际使用中，当描述**计算过程**时用 **\"multiplication\"**，当描述**结果**时用 **\"product\"**。如果还有疑问，随时联系我！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
