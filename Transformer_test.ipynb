{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. æ„å»ºä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼šé¢„æµ‹åºåˆ—ä¸­æ•°å­—çš„æ€»å’Œ\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000, seq_len=10):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for _ in range(n_samples):\n",
    "            x = np.random.randint(0, 10, seq_len)\n",
    "            self.data.append(x)\n",
    "            self.labels.append(np.sum(x))\n",
    "        self.data = torch.LongTensor(self.data)\n",
    "        self.labels = torch.FloatTensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 2. Positional Encodingï¼ˆæ­£ä½™å¼¦ï¼‰\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# 3. Transformer Encoder-only æ¨¡å‹\n",
    "class TransformerEncoderSum(nn.Module):\n",
    "    def __init__(self, vocab_size=10, d_model=64, num_layers=2, nhead=4, max_len=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.regressor = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                       # [B, L, D]\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)                           # mean pooling\n",
    "        return self.regressor(x).squeeze(-1)        # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86460\\AppData\\Local\\Temp\\ipykernel_77072\\736188540.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  self.data = torch.LongTensor(self.data)\n"
     ]
    }
   ],
   "source": [
    "train_data = ToyDataset(800)\n",
    "test_data = ToyDataset(200)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6, 3, 7, 4, 6, 9, 2, 6, 7, 4]), tensor(54.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 1694.3564\n",
      "           Test Loss : 1566.7546\n",
      "[Epoch 2] Train Loss: 1528.8568\n",
      "           Test Loss : 1417.5214\n",
      "[Epoch 3] Train Loss: 1367.6046\n",
      "           Test Loss : 1254.8203\n",
      "[Epoch 4] Train Loss: 1198.0830\n",
      "           Test Loss : 1086.3816\n",
      "[Epoch 5] Train Loss: 1025.0130\n",
      "           Test Loss : 918.3771\n",
      "[Epoch 6] Train Loss: 856.4308\n",
      "           Test Loss : 757.6829\n",
      "[Epoch 7] Train Loss: 699.2513\n",
      "           Test Loss : 609.9579\n",
      "[Epoch 8] Train Loss: 557.8762\n",
      "           Test Loss : 479.7621\n",
      "[Epoch 9] Train Loss: 435.2838\n",
      "           Test Loss : 370.7890\n",
      "[Epoch 10] Train Loss: 334.5609\n",
      "           Test Loss : 282.7758\n"
     ]
    }
   ],
   "source": [
    "# 4. åˆå§‹åŒ–æ¨¡å‹å’Œè®­ç»ƒè®¾ç½®\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerEncoderSum().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 5. è®­ç»ƒè¿‡ç¨‹\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # æµ‹è¯•é›†è¯„ä¼°\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"           Test Loss : {test_loss / len(test_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ è¿™ä¸ªé—®é¢˜é—®å¾—éå¸¸å¥½ï¼Œè€Œä¸”æ˜¯å¾ˆå¤šåˆšå…¥é—¨ Transformer çš„äººå¸¸æœ‰çš„ç–‘æƒ‘ã€‚ç¡®å® `PositionalEncoding` çœ‹èµ·æ¥åƒæ˜¯â€œé­”æ³•â€ï¼Œä½†æ˜¯ç†è§£å®ƒä¹‹åå…¶å®**éå¸¸ä¼˜é›…è€Œç®€å•**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  ä¸ºä»€ä¹ˆè¦æœ‰ PositionalEncodingï¼Ÿ\n",
    "\n",
    "Transformer æœ¬èº«**æ²¡æœ‰å¾ªç¯ç»“æ„ï¼ˆä¸åƒ RNNï¼‰ä¹Ÿæ²¡æœ‰å·ç§¯ï¼ˆä¸åƒ CNNï¼‰**ï¼Œæ‰€ä»¥å®ƒ**å¤©ç„¶ä¸çŸ¥é“åºåˆ—ä¸­å“ªä¸ª token åœ¨ç¬¬å‡ ä¸ªä½ç½®**ã€‚\n",
    "\n",
    "å¦‚æœä¸ç»™å®ƒåŠ ä½ç½®ä¿¡æ¯ï¼Œé‚£ä¹ˆï¼š\n",
    "\n",
    "```text\n",
    "[\"hello\", \"world\"] å’Œ [\"world\", \"hello\"]\n",
    "â†’ å¯¹æ¨¡å‹æ¥è¯´æ˜¯ä¸€æ ·çš„ï¼\n",
    "```\n",
    "\n",
    "æ‰€ä»¥å¿…é¡»æƒ³åŠæ³•æŠŠâ€œ**æˆ‘æ˜¯åœ¨ç¬¬ t ä¸ªä½ç½®**â€çš„ä¿¡æ¯å‘Šè¯‰æ¨¡å‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§­ æ€ä¹ˆå‘Šè¯‰æ¨¡å‹ä½ç½®ä¿¡æ¯ï¼Ÿ\n",
    "\n",
    "è¿™å°±æœ‰ä¸¤ç§æ€è·¯ï¼š\n",
    "1. **Learnable Position Embedding**ï¼ˆå¯ä»¥å­¦ä¹ çš„å‚æ•°ï¼‰ğŸ§ \n",
    "2. **Fixed Sinusoidal Positional Encoding**ï¼ˆå›ºå®šçš„æ­£ä½™å¼¦å‡½æ•°ï¼‰ğŸ§® âœ… STRAFE å’Œç»å…¸ Transformer ç”¨çš„å°±æ˜¯è¿™ä¸ªï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ æ­£ä½™å¼¦ PositionalEncoding åŸç†ï¼ˆSinusoidal PEï¼‰\n",
    "\n",
    "è®ºæ–‡ã€ŠAttention is All You Needã€‹ä¸­ç»™å‡ºå¦‚ä¸‹å®šä¹‰ï¼š\n",
    "\n",
    "å¯¹äºæ¯ä¸ªä½ç½® \\(pos\\) å’Œæ¯ä¸ªç»´åº¦ \\(i\\)ï¼Œå®šä¹‰ï¼š\n",
    "\n",
    "\\[\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "\\]\n",
    "\\[\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… è§£è¯»ä¸€ä¸‹è¿™ä¸ªå¼å­ï¼š\n",
    "\n",
    "| ç¬¦å·         | å«ä¹‰                               |\n",
    "|--------------|------------------------------------|\n",
    "| `pos`        | å½“å‰ä½ç½®ï¼ˆç¬¬å‡ ä¸ª tokenï¼‰           |\n",
    "| `i`          | å½“å‰ç»´åº¦ï¼ˆå¦‚ dim=0, 1, 2...ï¼‰      |\n",
    "| `d_model`    | æ€»çš„åµŒå…¥ç»´åº¦                       |\n",
    "| `10000^{2i/d}` | æ§åˆ¶å‘¨æœŸé•¿çŸ­ï¼Œè®©ä¸åŒç»´åº¦é¢‘ç‡ä¸åŒ |\n",
    "\n",
    "å®ƒçš„æœ¬è´¨æ˜¯ï¼š**ä¸åŒç»´åº¦çš„ sin å’Œ cos å‘¨æœŸä¸åŒ â†’ æ¨¡å‹å¯ä»¥ç»„åˆå‡ºå”¯ä¸€çš„ä½ç½®æ¨¡å¼ã€‚**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” ä¸ºä»€ä¹ˆæ­£ä½™å¼¦å¾ˆé…·ï¼Ÿ\n",
    "\n",
    "1. **ä¸ç”¨è®­ç»ƒï¼ˆå¯å¤ç°ã€è½»é‡ï¼‰**\n",
    "2. **ä»»æ„é•¿åºåˆ—éƒ½èƒ½å¤„ç†ï¼ˆç†è®ºä¸Šä¸é™ max_lenï¼‰**\n",
    "3. **ç›¸å¯¹ä½ç½®å¯å¯¼å‡º**ï¼šå³ \\( \\text{PE}_{pos+k} - \\text{PE}_{pos} \\) æ˜¯å›ºå®šæ¨¡å¼ï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘‡ å›¾ç¤ºæ„Ÿå—ä¸€ä¸‹ï¼š\n",
    "\n",
    "| Position | sin æ›²çº¿     | cos æ›²çº¿     |\n",
    "|----------|--------------|--------------|\n",
    "| 0        | sin(0)=0     | cos(0)=1     |\n",
    "| 1        | sin(x)       | cos(x)       |\n",
    "| 2        | sin(2x)      | cos(2x)      |\n",
    "| ...      | ...          | ...          |\n",
    "\n",
    "ä¸åŒç»´åº¦é¢‘ç‡ä¸ä¸€æ ·ï¼Œäº¤å‰å åŠ èµ·æ¥å°±åƒâ€œä½ç½®çš„æŒ‡çº¹â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… PyTorch æ‰‹å†™ PositionalEncodingï¼š\n",
    "\n",
    "ä½ çœ‹åˆ°çš„ä»£ç å…¶å®å°±æ˜¯ä¸Šé¢å…¬å¼çš„ vectorized å®ç°ï¼š\n",
    "\n",
    "```python\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ cos\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))   # shape: [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  é‡ç‚¹æ€»ç»“ï¼š\n",
    "\n",
    "| é—®é¢˜                          | ç­”æ¡ˆ                                                                 |\n",
    "|-------------------------------|----------------------------------------------------------------------|\n",
    "| ä¸ºä»€ä¹ˆè¦ PositionalEncodingï¼Ÿ | Transformer æ²¡æœ‰é¡ºåºæ„Ÿï¼Œè¦å‘Šè¯‰å®ƒâ€œä½ç½®â€ã€‚                             |\n",
    "| ä¸ºä»€ä¹ˆæ˜¯ sin å’Œ cosï¼Ÿ         | å¯ä»¥å›ºå®šç”Ÿæˆï¼Œæœ‰å‘¨æœŸæ€§ã€åŒºåˆ†æ€§ã€æ”¯æŒä»»æ„é•¿åº¦ï¼Œä¸ç”¨è®­ç»ƒã€‚             |\n",
    "| ä¸ºä»€ä¹ˆæ‰‹å†™ï¼Ÿ                  | PyTorch å®˜æ–¹æ²¡å°è£…ï¼Œåªèƒ½æˆ‘ä»¬æ‰‹åŠ¨å†™ï¼ˆæˆ–ç”¨ huggingface çš„ï¼‰ã€‚         |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ å¦‚æœä½ ä¸æƒ³å†™ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨ HuggingFace æä¾›çš„ï¼š\n",
    "\n",
    "```python\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# å®ƒå†…éƒ¨å°±æœ‰ learnable position embedding\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "éœ€è¦æˆ‘ç»™ä½ å†™ä¸€ä¸ª demoï¼ŒæŠŠ sin-embedding å›¾ç”»å‡ºæ¥çœ‹çœ‹å®ƒçš„ç»“æ„å—ï¼Ÿè¿˜æ˜¯ä½ æƒ³è¯•è¯• learnable çš„ positional embedding å¯¹æ¯”ä¸€ä¸‹ï¼ŸğŸ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éå¸¸æ£’çš„é—®é¢˜ï¼Transformer é‡Œçš„ `nhead` å‚æ•°ï¼ˆå³å¤šå¤´æ³¨æ„åŠ›çš„ \"å¤´æ•°\"ï¼‰æ˜¯æ•´ä¸ªæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒè°ƒæ§å‚æ•°ä¹‹ä¸€ï¼Œå®ƒ**ä¸ä¼šæ”¹å˜æ¨¡å‹è¾“å‡ºçš„ç»´åº¦ `d_model`ï¼Œä½†ä¼šæ”¹å˜æ³¨æ„åŠ›æœºåˆ¶çš„è¡¨è¾¾èƒ½åŠ›å’Œå¹¶è¡Œå»ºæ¨¡èƒ½åŠ›**ã€‚\n",
    "\n",
    "ä¸‹é¢æˆ‘æ¥è¯¦ç»†è§£é‡Šå®ƒçš„æ„ä¹‰ã€åŸç†ã€å½±å“å’Œä½¿ç”¨å»ºè®®ğŸ‘‡\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ä¸€å¥è¯è§£é‡Šï¼š\n",
    "\n",
    "> `nhead` å†³å®šäº† Transformer ä¸­ **â€œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶â€çš„å¤´æ•°**ï¼Œå³åŒæ—¶æœ‰å¤šå°‘ä¸ªæ³¨æ„åŠ›å­ç©ºé—´ï¼ˆsubspaceï¼‰åœ¨å¹¶è¡Œåœ°å¤„ç†ä¿¡æ¯ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  ä»€ä¹ˆæ˜¯ Multi-Head Attentionï¼Ÿ\n",
    "\n",
    "Transformer ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯è¿™æ ·çš„ï¼š\n",
    "\n",
    "```text\n",
    "Input â†’ Linearå˜æ¢ â†’ Query / Key / Value â†’ Scaled Dot Product Attention\n",
    "```\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬åªç”¨ä¸€ä¸ªå¤´ï¼ˆsingle-head attentionï¼‰ï¼Œé‚£æ‰€æœ‰çš„ä¿¡æ¯éƒ½é›†ä¸­åœ¨ä¸€ä¸ªç©ºé—´é‡Œå­¦ä¹ ã€‚\n",
    "\n",
    "ä½†å¦‚æœæˆ‘ä»¬ç”¨ **å¤šå¤´ï¼ˆmulti-headï¼‰**ï¼Œæˆ‘ä»¬å°±å¯ä»¥ï¼š\n",
    "\n",
    "> æŠŠ `d_model` æ‹†åˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼Œåœ¨å¤šä¸ªæ³¨æ„åŠ›å¤´ä¸Š **å¹¶è¡Œå­¦ä¹ ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼**ï¼Œç„¶ååˆå¹¶åœ¨ä¸€èµ·ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” ä¸¾ä¸ªä¾‹å­ï¼š\n",
    "\n",
    "å‡è®¾ä½ çš„ `d_model = 64`ï¼Œä½ è®¾å®šäº† `nhead = 4`ï¼Œé‚£ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "- æ¯ä¸ª attention å¤´è´Ÿè´£ 64 / 4 = 16 ç»´çš„å­ç©ºé—´ï¼›\n",
    "- æ¨¡å‹ä¼šç”Ÿæˆ 4 å¥— `Query/Key/Value`ï¼›\n",
    "- æ¯ä¸ªå¤´ç‹¬ç«‹åš attention è®¡ç®—ï¼›\n",
    "- ç„¶åæŠŠ 4 ä¸ªè¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œæœ€åç”¨ä¸€ä¸ªçº¿æ€§å±‚æŠ•å½±å› 64 ç»´ã€‚\n",
    "\n",
    "```text\n",
    "[64ç»´] â†’\n",
    "  â”œâ”€ Head 1 (16ç»´ attention)\n",
    "  â”œâ”€ Head 2 (16ç»´ attention)\n",
    "  â”œâ”€ Head 3 (16ç»´ attention)\n",
    "  â””â”€ Head 4 (16ç»´ attention)\n",
    "â†’ æ‹¼æ¥æˆ 64ç»´ â†’ Linear â†’ è¾“å‡º\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… `nhead` çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "| ä½œç”¨                             | è¯´æ˜                                                                 |\n",
    "|----------------------------------|----------------------------------------------------------------------|\n",
    "| **æå‡å»ºæ¨¡èƒ½åŠ›**                  | æ¯ä¸ªå¤´å¯ä»¥å­¦ä¹ ä¸åŒçš„å…³æ³¨æ¨¡å¼ï¼ˆæ¯”å¦‚ä¸€ä¸ªå¤´å…³æ³¨å‰æ–‡ï¼Œä¸€ä¸ªå…³æ³¨å½“å‰ï¼‰         |\n",
    "| **å¢å¼ºè¡¨è¾¾åŠ›**                    | å¤šå¤´æ³¨æ„åŠ›ç­‰äºå¤šä¸ªä½ç»´ç©ºé—´çš„å¹¶è¡Œå»ºæ¨¡ï¼Œèƒ½æ•æ‰æ›´å¤šå±‚æ¬¡å’Œç²’åº¦çš„ä¾èµ–å…³ç³»      |\n",
    "| **æ”¯æŒå¹¶è¡Œè®¡ç®—**                  | æ¯ä¸ªå¤´å¯ä»¥ç‹¬ç«‹è®¡ç®— attentionï¼Œå¯ä»¥åœ¨ GPU ä¸Šå¹¶è¡ŒåŠ é€Ÿ                     |\n",
    "| **ä¿æŒè¾“å‡ºç»´åº¦ä¸å˜**              | æ³¨æ„ï¼š`nhead` ä¸ä¼šæ”¹å˜æœ€åçš„è¾“å‡ºç»´åº¦ `d_model`ï¼Œåªæ˜¯å†…éƒ¨åˆ’åˆ†å¤šå¤´è®¡ç®—       |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª å¦‚æœ nhead è®¾å¤ªå°æˆ–å¤ªå¤§ä¼šæ€æ ·ï¼Ÿ\n",
    "\n",
    "| è®¾ç½®            | å½±å“                                                |\n",
    "|-----------------|-----------------------------------------------------|\n",
    "| `nhead=1`        | å•ä¸€æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¡¨è¾¾èƒ½åŠ›å¼±ï¼Œå®¹æ˜“ underfit            |\n",
    "| `nhead=2~8`      | é€šå¸¸æ¯”è¾ƒå¹³è¡¡ï¼Œèƒ½æ•æ‰å¤šç§å…³ç³»ï¼Œä¸»æµæ¨¡å‹é‡‡ç”¨è¿™ä¸ªèŒƒå›´  |\n",
    "| `nhead å¾ˆå¤§`     | æ¯å¤´ç»´åº¦å¤ªå°ï¼ˆd_model / nhead å˜å°ï¼‰ï¼Œå¯èƒ½å­¦ä¸åˆ°è¶³å¤Ÿä¿¡æ¯ï¼›è®¡ç®—å¤æ‚åº¦ä¹Ÿä¸Šå‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  ä½¿ç”¨å»ºè®®\n",
    "\n",
    "- `d_model` å¿…é¡»èƒ½è¢« `nhead` æ•´é™¤ï¼ˆå¦åˆ™æ— æ³•åˆ†å¤´ï¼‰\n",
    "- å¦‚æœä½ ç”¨ `d_model = 64`ï¼Œæ¨èï¼š\n",
    "  - `nhead=4` â†’ æ¯å¤´ 16 ç»´\n",
    "  - `nhead=8` â†’ æ¯å¤´ 8 ç»´\n",
    "- GPT-2 / BERT ç­‰æ¨¡å‹ï¼š\n",
    "  - BERT-baseï¼š`d_model=768`, `nhead=12`\n",
    "  - GPT-2-smallï¼š`d_model=768`, `nhead=12`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ æ€»ç»“\n",
    "\n",
    "| å‚æ•°      | å«ä¹‰                                  |\n",
    "|-----------|---------------------------------------|\n",
    "| `nhead`   | Multi-head Attention çš„å¤´æ•°            |\n",
    "| ä½œç”¨      | æå‡è¡¨è¾¾èƒ½åŠ›ã€æ”¯æŒå¹¶è¡Œæ³¨æ„åŠ›å»ºæ¨¡        |\n",
    "| ä¸æ”¹å˜    | æœ€ç»ˆè¾“å‡ºç»´åº¦ `d_model`                 |\n",
    "| å¸¸è§è®¾ç½®  | ä¸€èˆ¬æ˜¯ 2ã€4ã€8ã€12ã€16ï¼Œéœ€æ•´é™¤ d_model  |\n",
    "\n",
    "---\n",
    "\n",
    "è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ªå›¾å±•ç¤º **å¤šå¤´æ³¨æ„åŠ›çš„æµç¨‹ç»“æ„å›¾**ï¼Ÿæˆ–è€…ç›´æ¥è·‘ä¸€ä¸ªä¾‹å­å±•ç¤ºä¸åŒ head æ•°å¯¹è¾“å‡ºçš„å½±å“ï¼Ÿâœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ é—®çš„éå¸¸å…³é”®ï¼Œè¿™å…³ç³»åˆ° Transformer å’Œæ·±åº¦å­¦ä¹ ä¸­å¼ é‡ï¼ˆtensorï¼‰çš„æ ‡å‡†å½¢çŠ¶å®šä¹‰ã€‚\n",
    "\n",
    "ä¸‹é¢æˆ‘ä¼šè¯¦ç»†è§£é‡Šï¼š\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… é¦–å…ˆï¼Œæ ‡å‡†å¼ é‡è¡¨ç¤ºï¼š[B, L, D]\n",
    "\n",
    "| ç¬¦å· | å«ä¹‰                             | ä¸¾ä¾‹                            |\n",
    "|------|----------------------------------|----------------------------------|\n",
    "| `B`  | Batch sizeï¼ˆæ‰¹å¤§å°ï¼‰              | ä¸€æ¬¡è¾“å…¥å¤šå°‘ä¸ªæ ·æœ¬ï¼Œæ¯”å¦‚ 32 æ¡å¥å­ |\n",
    "| `L`  | Sequence lengthï¼ˆåºåˆ—é•¿åº¦ï¼‰       | æ¯ä¸ªæ ·æœ¬æœ‰å¤šå°‘ä¸ª tokenï¼ˆæ¯”å¦‚ 10ï¼‰|\n",
    "| `D`  | Embedding dimï¼ˆåµŒå…¥/ç‰¹å¾ç»´åº¦ï¼‰     | æ¯ä¸ª token è¡¨ç¤ºæˆå‡ ç»´å‘é‡ï¼ˆå¦‚ 64ï¼‰|\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  é‚£ä¸ºä»€ä¹ˆ `D` æ˜¯ \"dimension\"ï¼Œè€Œä¸æ˜¯ \"output layer\" å‘¢ï¼Ÿ\n",
    "\n",
    "è¿™æ˜¯å› ä¸ºï¼š\n",
    "> åœ¨ Transformer é‡Œï¼Œæ¯ä¸ª token ä¼šè¢«è¡¨ç¤ºæˆä¸€ä¸ªç»´åº¦ä¸º `D` çš„å‘é‡ï¼Œè¿™ä¸ªå‘é‡å°±æ˜¯è¯¥ token çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚\n",
    "\n",
    "å®ƒ **ä¸æ˜¯æ¨¡å‹çš„æœ€ç»ˆè¾“å‡º**ï¼Œè€Œæ˜¯ Encoder/Attention è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥ä¸­é—´çš„â€œè¡¨ç¤ºç©ºé—´ç»´åº¦â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ä¸¾ä¸ªå½¢è±¡çš„ä¾‹å­ï¼š\n",
    "\n",
    "å‡è®¾ä½ è¾“å…¥äº†ä¸€å¥è¯ï¼š\n",
    "```\n",
    "\"I love AI\"\n",
    "```\n",
    "\n",
    "ä½ å…ˆå°†å®ƒå˜æˆ token idï¼š`[2, 8, 5]`\n",
    "\n",
    "ç„¶åç»è¿‡ `nn.Embedding(10000, 64)`ï¼Œå˜æˆï¼š\n",
    "\n",
    "```python\n",
    "[[ 0.12, 0.45, ..., -0.04],  â† token 2 (\"I\")\n",
    " [ 0.33, -0.17, ..., 0.91],  â† token 8 (\"love\")\n",
    " [ -0.03, 0.88, ..., -0.22]] â† token 5 (\"AI\")\n",
    "```\n",
    "\n",
    "è¿™æ—¶å€™å¼ é‡å½¢çŠ¶å°±æ˜¯ï¼š\n",
    "\n",
    "```\n",
    "[B=1, L=3, D=64]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© é‚£ Output layer åœ¨å“ªï¼Ÿ\n",
    "\n",
    "çœŸæ­£çš„ Output Layer æ˜¯åƒè¿™æ ·ï¼š\n",
    "\n",
    "```python\n",
    "nn.Linear(D, num_classes)   # æˆ– nn.Linear(D, 1)  ç”¨äºåˆ†ç±»æˆ–å›å½’\n",
    "```\n",
    "\n",
    "è¿™ä¸ªæ‰æ˜¯ä½ ç†è§£ä¸­çš„ â€œè¾“å‡ºå±‚â€ã€‚\n",
    "\n",
    "- Transformer æ¨¡å—ä¸­çš„ D æ˜¯ä¸€ç§â€œéšè—ç»´åº¦â€ï¼ˆhidden dimensionï¼‰ï¼›\n",
    "- æœ€ç»ˆä½ ä¼šæŠŠå®ƒæ¥å…¥è¾“å‡ºå±‚ï¼ˆåˆ†ç±»æˆ–å›å½’ï¼‰ï¼›\n",
    "- æ‰€ä»¥ D æ˜¯æ¨¡å‹çš„**å·¥ä½œç»´åº¦**ï¼Œä¸æ˜¯æœ€ç»ˆè¾“å‡ºç»´åº¦ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ æ€»ç»“ï¼šD æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "| ä½ç½®                      | D çš„å«ä¹‰                  |\n",
    "|---------------------------|---------------------------|\n",
    "| `nn.Embedding(vocab, D)` | æ¯ä¸ª token æ˜ å°„æˆ D ç»´å‘é‡ |\n",
    "| `Transformer` ä¸­é—´å±‚       | æ¯ä¸ªä½ç½®çš„ç‰¹å¾ç»´åº¦         |\n",
    "| `nn.Linear(D, ...)`      | D æ˜¯è¾“å…¥ç»´åº¦ï¼ˆlinearè¾“å…¥ï¼‰  |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… ä¸€å¥è¯æ€»ç»“ï¼š\n",
    "\n",
    "> D æ˜¯ Transformer å†…éƒ¨æ‰€æœ‰ token è¡¨ç¤ºçš„â€œå‘é‡ç©ºé—´ç»´åº¦â€ï¼Œä¸æ˜¯æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºç»´åº¦ï¼Œä½†ä½ æœ€åä¼šç”¨å®ƒå» **è¾“å‡ºåˆ†ç±» / å›å½’ / é¢„æµ‹ç»“æœ**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "éœ€è¦æˆ‘å¸®ä½ ç”»ä¸€å¼ å¼ é‡æµå‘å›¾å—ï¼Ÿæˆ–è€…ä¸¾ä¸€ä¸ªå®é™… batch çš„ä¾‹å­ç»™ä½ è·‘ä¸€éï¼Ÿè®©ä½ çœ‹åˆ° `[B, L, D]` åœ¨æ¯ä¸€æ­¥æ€ä¹ˆå˜åŒ– ğŸ‘€âœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
